{"version":3,"file":"comunica-browser.js","sources":["webpack://Comunica/webpack/bootstrap","webpack://Comunica//home/travis/build/comunica/comunica/node_modules/buffer/index.js","webpack://Comunica//home/travis/build/comunica/comunica/node_modules/jsonld/lib/context.js","webpack://Comunica//home/travis/build/comunica/comunica/node_modules/tslib/tslib.es6.js","webpack://Comunica//home/travis/build/comunica/comunica/node_modules/jsonld/lib/jsonld.js","webpack://Comunica/../actor-http-native/lib/Requester-browser.js","webpack://Comunica//home/travis/build/comunica/comunica/node_modules/punycode/punycode.js","webpack://Comunica//home/travis/build/comunica/comunica/node_modules/jsonld/lib/compact.js"],"sourcesContent":[" \t// The module cache\n \tvar installedModules = {};\n\n \t// The require function\n \tfunction __webpack_require__(moduleId) {\n\n \t\t// Check if module is in cache\n \t\tif(installedModules[moduleId]) {\n \t\t\treturn installedModules[moduleId].exports;\n \t\t}\n \t\t// Create a new module (and put it into the cache)\n \t\tvar module = installedModules[moduleId] = {\n \t\t\ti: moduleId,\n \t\t\tl: false,\n \t\t\texports: {}\n \t\t};\n\n \t\t// Execute the module function\n \t\tmodules[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n \t\t// Flag the module as loaded\n \t\tmodule.l = true;\n\n \t\t// Return the exports of the module\n \t\treturn module.exports;\n \t}\n\n\n \t// expose the modules object (__webpack_modules__)\n \t__webpack_require__.m = modules;\n\n \t// expose the module cache\n \t__webpack_require__.c = installedModules;\n\n \t// define getter function for harmony exports\n \t__webpack_require__.d = function(exports, name, getter) {\n \t\tif(!__webpack_require__.o(exports, name)) {\n \t\t\tObject.defineProperty(exports, name, { enumerable: true, get: getter });\n \t\t}\n \t};\n\n \t// define __esModule on exports\n \t__webpack_require__.r = function(exports) {\n \t\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n \t\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n \t\t}\n \t\tObject.defineProperty(exports, '__esModule', { value: true });\n \t};\n\n \t// create a fake namespace object\n \t// mode & 1: value is a module id, require it\n \t// mode & 2: merge all properties of value into the ns\n \t// mode & 4: return value when already ns object\n \t// mode & 8|1: behave like require\n \t__webpack_require__.t = function(value, mode) {\n \t\tif(mode & 1) value = __webpack_require__(value);\n \t\tif(mode & 8) return value;\n \t\tif((mode & 4) && typeof value === 'object' && value && value.__esModule) return value;\n \t\tvar ns = Object.create(null);\n \t\t__webpack_require__.r(ns);\n \t\tObject.defineProperty(ns, 'default', { enumerable: true, value: value });\n \t\tif(mode & 2 && typeof value != 'string') for(var key in value) __webpack_require__.d(ns, key, function(key) { return value[key]; }.bind(null, key));\n \t\treturn ns;\n \t};\n\n \t// getDefaultExport function for compatibility with non-harmony modules\n \t__webpack_require__.n = function(module) {\n \t\tvar getter = module && module.__esModule ?\n \t\t\tfunction getDefault() { return module['default']; } :\n \t\t\tfunction getModuleExports() { return module; };\n \t\t__webpack_require__.d(getter, 'a', getter);\n \t\treturn getter;\n \t};\n\n \t// Object.prototype.hasOwnProperty.call\n \t__webpack_require__.o = function(object, property) { return Object.prototype.hasOwnProperty.call(object, property); };\n\n \t// __webpack_public_path__\n \t__webpack_require__.p = \"\";\n\n\n \t// Load entry module and return exports\n \treturn __webpack_require__(__webpack_require__.s = 288);\n","/*!\n * The buffer module from node.js, for the browser.\n *\n * @author   Feross Aboukhadijeh <feross@feross.org> <http://feross.org>\n * @license  MIT\n */\n/* eslint-disable no-proto */\n\n'use strict'\n\nvar base64 = require('base64-js')\nvar ieee754 = require('ieee754')\nvar isArray = require('isarray')\n\nexports.Buffer = Buffer\nexports.SlowBuffer = SlowBuffer\nexports.INSPECT_MAX_BYTES = 50\n\n/**\n * If `Buffer.TYPED_ARRAY_SUPPORT`:\n *   === true    Use Uint8Array implementation (fastest)\n *   === false   Use Object implementation (most compatible, even IE6)\n *\n * Browsers that support typed arrays are IE 10+, Firefox 4+, Chrome 7+, Safari 5.1+,\n * Opera 11.6+, iOS 4.2+.\n *\n * Due to various browser bugs, sometimes the Object implementation will be used even\n * when the browser supports typed arrays.\n *\n * Note:\n *\n *   - Firefox 4-29 lacks support for adding new properties to `Uint8Array` instances,\n *     See: https://bugzilla.mozilla.org/show_bug.cgi?id=695438.\n *\n *   - Chrome 9-10 is missing the `TypedArray.prototype.subarray` function.\n *\n *   - IE10 has a broken `TypedArray.prototype.subarray` function which returns arrays of\n *     incorrect length in some situations.\n\n * We detect these buggy browsers and set `Buffer.TYPED_ARRAY_SUPPORT` to `false` so they\n * get the Object implementation, which is slower but behaves correctly.\n */\nBuffer.TYPED_ARRAY_SUPPORT = global.TYPED_ARRAY_SUPPORT !== undefined\n  ? global.TYPED_ARRAY_SUPPORT\n  : typedArraySupport()\n\n/*\n * Export kMaxLength after typed array support is determined.\n */\nexports.kMaxLength = kMaxLength()\n\nfunction typedArraySupport () {\n  try {\n    var arr = new Uint8Array(1)\n    arr.__proto__ = {__proto__: Uint8Array.prototype, foo: function () { return 42 }}\n    return arr.foo() === 42 && // typed array instances can be augmented\n        typeof arr.subarray === 'function' && // chrome 9-10 lack `subarray`\n        arr.subarray(1, 1).byteLength === 0 // ie10 has broken `subarray`\n  } catch (e) {\n    return false\n  }\n}\n\nfunction kMaxLength () {\n  return Buffer.TYPED_ARRAY_SUPPORT\n    ? 0x7fffffff\n    : 0x3fffffff\n}\n\nfunction createBuffer (that, length) {\n  if (kMaxLength() < length) {\n    throw new RangeError('Invalid typed array length')\n  }\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    // Return an augmented `Uint8Array` instance, for best performance\n    that = new Uint8Array(length)\n    that.__proto__ = Buffer.prototype\n  } else {\n    // Fallback: Return an object instance of the Buffer class\n    if (that === null) {\n      that = new Buffer(length)\n    }\n    that.length = length\n  }\n\n  return that\n}\n\n/**\n * The Buffer constructor returns instances of `Uint8Array` that have their\n * prototype changed to `Buffer.prototype`. Furthermore, `Buffer` is a subclass of\n * `Uint8Array`, so the returned instances will have all the node `Buffer` methods\n * and the `Uint8Array` methods. Square bracket notation works as expected -- it\n * returns a single octet.\n *\n * The `Uint8Array` prototype remains unmodified.\n */\n\nfunction Buffer (arg, encodingOrOffset, length) {\n  if (!Buffer.TYPED_ARRAY_SUPPORT && !(this instanceof Buffer)) {\n    return new Buffer(arg, encodingOrOffset, length)\n  }\n\n  // Common case.\n  if (typeof arg === 'number') {\n    if (typeof encodingOrOffset === 'string') {\n      throw new Error(\n        'If encoding is specified then the first argument must be a string'\n      )\n    }\n    return allocUnsafe(this, arg)\n  }\n  return from(this, arg, encodingOrOffset, length)\n}\n\nBuffer.poolSize = 8192 // not used by this implementation\n\n// TODO: Legacy, not needed anymore. Remove in next major version.\nBuffer._augment = function (arr) {\n  arr.__proto__ = Buffer.prototype\n  return arr\n}\n\nfunction from (that, value, encodingOrOffset, length) {\n  if (typeof value === 'number') {\n    throw new TypeError('\"value\" argument must not be a number')\n  }\n\n  if (typeof ArrayBuffer !== 'undefined' && value instanceof ArrayBuffer) {\n    return fromArrayBuffer(that, value, encodingOrOffset, length)\n  }\n\n  if (typeof value === 'string') {\n    return fromString(that, value, encodingOrOffset)\n  }\n\n  return fromObject(that, value)\n}\n\n/**\n * Functionally equivalent to Buffer(arg, encoding) but throws a TypeError\n * if value is a number.\n * Buffer.from(str[, encoding])\n * Buffer.from(array)\n * Buffer.from(buffer)\n * Buffer.from(arrayBuffer[, byteOffset[, length]])\n **/\nBuffer.from = function (value, encodingOrOffset, length) {\n  return from(null, value, encodingOrOffset, length)\n}\n\nif (Buffer.TYPED_ARRAY_SUPPORT) {\n  Buffer.prototype.__proto__ = Uint8Array.prototype\n  Buffer.__proto__ = Uint8Array\n  if (typeof Symbol !== 'undefined' && Symbol.species &&\n      Buffer[Symbol.species] === Buffer) {\n    // Fix subarray() in ES2016. See: https://github.com/feross/buffer/pull/97\n    Object.defineProperty(Buffer, Symbol.species, {\n      value: null,\n      configurable: true\n    })\n  }\n}\n\nfunction assertSize (size) {\n  if (typeof size !== 'number') {\n    throw new TypeError('\"size\" argument must be a number')\n  } else if (size < 0) {\n    throw new RangeError('\"size\" argument must not be negative')\n  }\n}\n\nfunction alloc (that, size, fill, encoding) {\n  assertSize(size)\n  if (size <= 0) {\n    return createBuffer(that, size)\n  }\n  if (fill !== undefined) {\n    // Only pay attention to encoding if it's a string. This\n    // prevents accidentally sending in a number that would\n    // be interpretted as a start offset.\n    return typeof encoding === 'string'\n      ? createBuffer(that, size).fill(fill, encoding)\n      : createBuffer(that, size).fill(fill)\n  }\n  return createBuffer(that, size)\n}\n\n/**\n * Creates a new filled Buffer instance.\n * alloc(size[, fill[, encoding]])\n **/\nBuffer.alloc = function (size, fill, encoding) {\n  return alloc(null, size, fill, encoding)\n}\n\nfunction allocUnsafe (that, size) {\n  assertSize(size)\n  that = createBuffer(that, size < 0 ? 0 : checked(size) | 0)\n  if (!Buffer.TYPED_ARRAY_SUPPORT) {\n    for (var i = 0; i < size; ++i) {\n      that[i] = 0\n    }\n  }\n  return that\n}\n\n/**\n * Equivalent to Buffer(num), by default creates a non-zero-filled Buffer instance.\n * */\nBuffer.allocUnsafe = function (size) {\n  return allocUnsafe(null, size)\n}\n/**\n * Equivalent to SlowBuffer(num), by default creates a non-zero-filled Buffer instance.\n */\nBuffer.allocUnsafeSlow = function (size) {\n  return allocUnsafe(null, size)\n}\n\nfunction fromString (that, string, encoding) {\n  if (typeof encoding !== 'string' || encoding === '') {\n    encoding = 'utf8'\n  }\n\n  if (!Buffer.isEncoding(encoding)) {\n    throw new TypeError('\"encoding\" must be a valid string encoding')\n  }\n\n  var length = byteLength(string, encoding) | 0\n  that = createBuffer(that, length)\n\n  var actual = that.write(string, encoding)\n\n  if (actual !== length) {\n    // Writing a hex string, for example, that contains invalid characters will\n    // cause everything after the first invalid character to be ignored. (e.g.\n    // 'abxxcd' will be treated as 'ab')\n    that = that.slice(0, actual)\n  }\n\n  return that\n}\n\nfunction fromArrayLike (that, array) {\n  var length = array.length < 0 ? 0 : checked(array.length) | 0\n  that = createBuffer(that, length)\n  for (var i = 0; i < length; i += 1) {\n    that[i] = array[i] & 255\n  }\n  return that\n}\n\nfunction fromArrayBuffer (that, array, byteOffset, length) {\n  array.byteLength // this throws if `array` is not a valid ArrayBuffer\n\n  if (byteOffset < 0 || array.byteLength < byteOffset) {\n    throw new RangeError('\\'offset\\' is out of bounds')\n  }\n\n  if (array.byteLength < byteOffset + (length || 0)) {\n    throw new RangeError('\\'length\\' is out of bounds')\n  }\n\n  if (byteOffset === undefined && length === undefined) {\n    array = new Uint8Array(array)\n  } else if (length === undefined) {\n    array = new Uint8Array(array, byteOffset)\n  } else {\n    array = new Uint8Array(array, byteOffset, length)\n  }\n\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    // Return an augmented `Uint8Array` instance, for best performance\n    that = array\n    that.__proto__ = Buffer.prototype\n  } else {\n    // Fallback: Return an object instance of the Buffer class\n    that = fromArrayLike(that, array)\n  }\n  return that\n}\n\nfunction fromObject (that, obj) {\n  if (Buffer.isBuffer(obj)) {\n    var len = checked(obj.length) | 0\n    that = createBuffer(that, len)\n\n    if (that.length === 0) {\n      return that\n    }\n\n    obj.copy(that, 0, 0, len)\n    return that\n  }\n\n  if (obj) {\n    if ((typeof ArrayBuffer !== 'undefined' &&\n        obj.buffer instanceof ArrayBuffer) || 'length' in obj) {\n      if (typeof obj.length !== 'number' || isnan(obj.length)) {\n        return createBuffer(that, 0)\n      }\n      return fromArrayLike(that, obj)\n    }\n\n    if (obj.type === 'Buffer' && isArray(obj.data)) {\n      return fromArrayLike(that, obj.data)\n    }\n  }\n\n  throw new TypeError('First argument must be a string, Buffer, ArrayBuffer, Array, or array-like object.')\n}\n\nfunction checked (length) {\n  // Note: cannot use `length < kMaxLength()` here because that fails when\n  // length is NaN (which is otherwise coerced to zero.)\n  if (length >= kMaxLength()) {\n    throw new RangeError('Attempt to allocate Buffer larger than maximum ' +\n                         'size: 0x' + kMaxLength().toString(16) + ' bytes')\n  }\n  return length | 0\n}\n\nfunction SlowBuffer (length) {\n  if (+length != length) { // eslint-disable-line eqeqeq\n    length = 0\n  }\n  return Buffer.alloc(+length)\n}\n\nBuffer.isBuffer = function isBuffer (b) {\n  return !!(b != null && b._isBuffer)\n}\n\nBuffer.compare = function compare (a, b) {\n  if (!Buffer.isBuffer(a) || !Buffer.isBuffer(b)) {\n    throw new TypeError('Arguments must be Buffers')\n  }\n\n  if (a === b) return 0\n\n  var x = a.length\n  var y = b.length\n\n  for (var i = 0, len = Math.min(x, y); i < len; ++i) {\n    if (a[i] !== b[i]) {\n      x = a[i]\n      y = b[i]\n      break\n    }\n  }\n\n  if (x < y) return -1\n  if (y < x) return 1\n  return 0\n}\n\nBuffer.isEncoding = function isEncoding (encoding) {\n  switch (String(encoding).toLowerCase()) {\n    case 'hex':\n    case 'utf8':\n    case 'utf-8':\n    case 'ascii':\n    case 'latin1':\n    case 'binary':\n    case 'base64':\n    case 'ucs2':\n    case 'ucs-2':\n    case 'utf16le':\n    case 'utf-16le':\n      return true\n    default:\n      return false\n  }\n}\n\nBuffer.concat = function concat (list, length) {\n  if (!isArray(list)) {\n    throw new TypeError('\"list\" argument must be an Array of Buffers')\n  }\n\n  if (list.length === 0) {\n    return Buffer.alloc(0)\n  }\n\n  var i\n  if (length === undefined) {\n    length = 0\n    for (i = 0; i < list.length; ++i) {\n      length += list[i].length\n    }\n  }\n\n  var buffer = Buffer.allocUnsafe(length)\n  var pos = 0\n  for (i = 0; i < list.length; ++i) {\n    var buf = list[i]\n    if (!Buffer.isBuffer(buf)) {\n      throw new TypeError('\"list\" argument must be an Array of Buffers')\n    }\n    buf.copy(buffer, pos)\n    pos += buf.length\n  }\n  return buffer\n}\n\nfunction byteLength (string, encoding) {\n  if (Buffer.isBuffer(string)) {\n    return string.length\n  }\n  if (typeof ArrayBuffer !== 'undefined' && typeof ArrayBuffer.isView === 'function' &&\n      (ArrayBuffer.isView(string) || string instanceof ArrayBuffer)) {\n    return string.byteLength\n  }\n  if (typeof string !== 'string') {\n    string = '' + string\n  }\n\n  var len = string.length\n  if (len === 0) return 0\n\n  // Use a for loop to avoid recursion\n  var loweredCase = false\n  for (;;) {\n    switch (encoding) {\n      case 'ascii':\n      case 'latin1':\n      case 'binary':\n        return len\n      case 'utf8':\n      case 'utf-8':\n      case undefined:\n        return utf8ToBytes(string).length\n      case 'ucs2':\n      case 'ucs-2':\n      case 'utf16le':\n      case 'utf-16le':\n        return len * 2\n      case 'hex':\n        return len >>> 1\n      case 'base64':\n        return base64ToBytes(string).length\n      default:\n        if (loweredCase) return utf8ToBytes(string).length // assume utf8\n        encoding = ('' + encoding).toLowerCase()\n        loweredCase = true\n    }\n  }\n}\nBuffer.byteLength = byteLength\n\nfunction slowToString (encoding, start, end) {\n  var loweredCase = false\n\n  // No need to verify that \"this.length <= MAX_UINT32\" since it's a read-only\n  // property of a typed array.\n\n  // This behaves neither like String nor Uint8Array in that we set start/end\n  // to their upper/lower bounds if the value passed is out of range.\n  // undefined is handled specially as per ECMA-262 6th Edition,\n  // Section 13.3.3.7 Runtime Semantics: KeyedBindingInitialization.\n  if (start === undefined || start < 0) {\n    start = 0\n  }\n  // Return early if start > this.length. Done here to prevent potential uint32\n  // coercion fail below.\n  if (start > this.length) {\n    return ''\n  }\n\n  if (end === undefined || end > this.length) {\n    end = this.length\n  }\n\n  if (end <= 0) {\n    return ''\n  }\n\n  // Force coersion to uint32. This will also coerce falsey/NaN values to 0.\n  end >>>= 0\n  start >>>= 0\n\n  if (end <= start) {\n    return ''\n  }\n\n  if (!encoding) encoding = 'utf8'\n\n  while (true) {\n    switch (encoding) {\n      case 'hex':\n        return hexSlice(this, start, end)\n\n      case 'utf8':\n      case 'utf-8':\n        return utf8Slice(this, start, end)\n\n      case 'ascii':\n        return asciiSlice(this, start, end)\n\n      case 'latin1':\n      case 'binary':\n        return latin1Slice(this, start, end)\n\n      case 'base64':\n        return base64Slice(this, start, end)\n\n      case 'ucs2':\n      case 'ucs-2':\n      case 'utf16le':\n      case 'utf-16le':\n        return utf16leSlice(this, start, end)\n\n      default:\n        if (loweredCase) throw new TypeError('Unknown encoding: ' + encoding)\n        encoding = (encoding + '').toLowerCase()\n        loweredCase = true\n    }\n  }\n}\n\n// The property is used by `Buffer.isBuffer` and `is-buffer` (in Safari 5-7) to detect\n// Buffer instances.\nBuffer.prototype._isBuffer = true\n\nfunction swap (b, n, m) {\n  var i = b[n]\n  b[n] = b[m]\n  b[m] = i\n}\n\nBuffer.prototype.swap16 = function swap16 () {\n  var len = this.length\n  if (len % 2 !== 0) {\n    throw new RangeError('Buffer size must be a multiple of 16-bits')\n  }\n  for (var i = 0; i < len; i += 2) {\n    swap(this, i, i + 1)\n  }\n  return this\n}\n\nBuffer.prototype.swap32 = function swap32 () {\n  var len = this.length\n  if (len % 4 !== 0) {\n    throw new RangeError('Buffer size must be a multiple of 32-bits')\n  }\n  for (var i = 0; i < len; i += 4) {\n    swap(this, i, i + 3)\n    swap(this, i + 1, i + 2)\n  }\n  return this\n}\n\nBuffer.prototype.swap64 = function swap64 () {\n  var len = this.length\n  if (len % 8 !== 0) {\n    throw new RangeError('Buffer size must be a multiple of 64-bits')\n  }\n  for (var i = 0; i < len; i += 8) {\n    swap(this, i, i + 7)\n    swap(this, i + 1, i + 6)\n    swap(this, i + 2, i + 5)\n    swap(this, i + 3, i + 4)\n  }\n  return this\n}\n\nBuffer.prototype.toString = function toString () {\n  var length = this.length | 0\n  if (length === 0) return ''\n  if (arguments.length === 0) return utf8Slice(this, 0, length)\n  return slowToString.apply(this, arguments)\n}\n\nBuffer.prototype.equals = function equals (b) {\n  if (!Buffer.isBuffer(b)) throw new TypeError('Argument must be a Buffer')\n  if (this === b) return true\n  return Buffer.compare(this, b) === 0\n}\n\nBuffer.prototype.inspect = function inspect () {\n  var str = ''\n  var max = exports.INSPECT_MAX_BYTES\n  if (this.length > 0) {\n    str = this.toString('hex', 0, max).match(/.{2}/g).join(' ')\n    if (this.length > max) str += ' ... '\n  }\n  return '<Buffer ' + str + '>'\n}\n\nBuffer.prototype.compare = function compare (target, start, end, thisStart, thisEnd) {\n  if (!Buffer.isBuffer(target)) {\n    throw new TypeError('Argument must be a Buffer')\n  }\n\n  if (start === undefined) {\n    start = 0\n  }\n  if (end === undefined) {\n    end = target ? target.length : 0\n  }\n  if (thisStart === undefined) {\n    thisStart = 0\n  }\n  if (thisEnd === undefined) {\n    thisEnd = this.length\n  }\n\n  if (start < 0 || end > target.length || thisStart < 0 || thisEnd > this.length) {\n    throw new RangeError('out of range index')\n  }\n\n  if (thisStart >= thisEnd && start >= end) {\n    return 0\n  }\n  if (thisStart >= thisEnd) {\n    return -1\n  }\n  if (start >= end) {\n    return 1\n  }\n\n  start >>>= 0\n  end >>>= 0\n  thisStart >>>= 0\n  thisEnd >>>= 0\n\n  if (this === target) return 0\n\n  var x = thisEnd - thisStart\n  var y = end - start\n  var len = Math.min(x, y)\n\n  var thisCopy = this.slice(thisStart, thisEnd)\n  var targetCopy = target.slice(start, end)\n\n  for (var i = 0; i < len; ++i) {\n    if (thisCopy[i] !== targetCopy[i]) {\n      x = thisCopy[i]\n      y = targetCopy[i]\n      break\n    }\n  }\n\n  if (x < y) return -1\n  if (y < x) return 1\n  return 0\n}\n\n// Finds either the first index of `val` in `buffer` at offset >= `byteOffset`,\n// OR the last index of `val` in `buffer` at offset <= `byteOffset`.\n//\n// Arguments:\n// - buffer - a Buffer to search\n// - val - a string, Buffer, or number\n// - byteOffset - an index into `buffer`; will be clamped to an int32\n// - encoding - an optional encoding, relevant is val is a string\n// - dir - true for indexOf, false for lastIndexOf\nfunction bidirectionalIndexOf (buffer, val, byteOffset, encoding, dir) {\n  // Empty buffer means no match\n  if (buffer.length === 0) return -1\n\n  // Normalize byteOffset\n  if (typeof byteOffset === 'string') {\n    encoding = byteOffset\n    byteOffset = 0\n  } else if (byteOffset > 0x7fffffff) {\n    byteOffset = 0x7fffffff\n  } else if (byteOffset < -0x80000000) {\n    byteOffset = -0x80000000\n  }\n  byteOffset = +byteOffset  // Coerce to Number.\n  if (isNaN(byteOffset)) {\n    // byteOffset: it it's undefined, null, NaN, \"foo\", etc, search whole buffer\n    byteOffset = dir ? 0 : (buffer.length - 1)\n  }\n\n  // Normalize byteOffset: negative offsets start from the end of the buffer\n  if (byteOffset < 0) byteOffset = buffer.length + byteOffset\n  if (byteOffset >= buffer.length) {\n    if (dir) return -1\n    else byteOffset = buffer.length - 1\n  } else if (byteOffset < 0) {\n    if (dir) byteOffset = 0\n    else return -1\n  }\n\n  // Normalize val\n  if (typeof val === 'string') {\n    val = Buffer.from(val, encoding)\n  }\n\n  // Finally, search either indexOf (if dir is true) or lastIndexOf\n  if (Buffer.isBuffer(val)) {\n    // Special case: looking for empty string/buffer always fails\n    if (val.length === 0) {\n      return -1\n    }\n    return arrayIndexOf(buffer, val, byteOffset, encoding, dir)\n  } else if (typeof val === 'number') {\n    val = val & 0xFF // Search for a byte value [0-255]\n    if (Buffer.TYPED_ARRAY_SUPPORT &&\n        typeof Uint8Array.prototype.indexOf === 'function') {\n      if (dir) {\n        return Uint8Array.prototype.indexOf.call(buffer, val, byteOffset)\n      } else {\n        return Uint8Array.prototype.lastIndexOf.call(buffer, val, byteOffset)\n      }\n    }\n    return arrayIndexOf(buffer, [ val ], byteOffset, encoding, dir)\n  }\n\n  throw new TypeError('val must be string, number or Buffer')\n}\n\nfunction arrayIndexOf (arr, val, byteOffset, encoding, dir) {\n  var indexSize = 1\n  var arrLength = arr.length\n  var valLength = val.length\n\n  if (encoding !== undefined) {\n    encoding = String(encoding).toLowerCase()\n    if (encoding === 'ucs2' || encoding === 'ucs-2' ||\n        encoding === 'utf16le' || encoding === 'utf-16le') {\n      if (arr.length < 2 || val.length < 2) {\n        return -1\n      }\n      indexSize = 2\n      arrLength /= 2\n      valLength /= 2\n      byteOffset /= 2\n    }\n  }\n\n  function read (buf, i) {\n    if (indexSize === 1) {\n      return buf[i]\n    } else {\n      return buf.readUInt16BE(i * indexSize)\n    }\n  }\n\n  var i\n  if (dir) {\n    var foundIndex = -1\n    for (i = byteOffset; i < arrLength; i++) {\n      if (read(arr, i) === read(val, foundIndex === -1 ? 0 : i - foundIndex)) {\n        if (foundIndex === -1) foundIndex = i\n        if (i - foundIndex + 1 === valLength) return foundIndex * indexSize\n      } else {\n        if (foundIndex !== -1) i -= i - foundIndex\n        foundIndex = -1\n      }\n    }\n  } else {\n    if (byteOffset + valLength > arrLength) byteOffset = arrLength - valLength\n    for (i = byteOffset; i >= 0; i--) {\n      var found = true\n      for (var j = 0; j < valLength; j++) {\n        if (read(arr, i + j) !== read(val, j)) {\n          found = false\n          break\n        }\n      }\n      if (found) return i\n    }\n  }\n\n  return -1\n}\n\nBuffer.prototype.includes = function includes (val, byteOffset, encoding) {\n  return this.indexOf(val, byteOffset, encoding) !== -1\n}\n\nBuffer.prototype.indexOf = function indexOf (val, byteOffset, encoding) {\n  return bidirectionalIndexOf(this, val, byteOffset, encoding, true)\n}\n\nBuffer.prototype.lastIndexOf = function lastIndexOf (val, byteOffset, encoding) {\n  return bidirectionalIndexOf(this, val, byteOffset, encoding, false)\n}\n\nfunction hexWrite (buf, string, offset, length) {\n  offset = Number(offset) || 0\n  var remaining = buf.length - offset\n  if (!length) {\n    length = remaining\n  } else {\n    length = Number(length)\n    if (length > remaining) {\n      length = remaining\n    }\n  }\n\n  // must be an even number of digits\n  var strLen = string.length\n  if (strLen % 2 !== 0) throw new TypeError('Invalid hex string')\n\n  if (length > strLen / 2) {\n    length = strLen / 2\n  }\n  for (var i = 0; i < length; ++i) {\n    var parsed = parseInt(string.substr(i * 2, 2), 16)\n    if (isNaN(parsed)) return i\n    buf[offset + i] = parsed\n  }\n  return i\n}\n\nfunction utf8Write (buf, string, offset, length) {\n  return blitBuffer(utf8ToBytes(string, buf.length - offset), buf, offset, length)\n}\n\nfunction asciiWrite (buf, string, offset, length) {\n  return blitBuffer(asciiToBytes(string), buf, offset, length)\n}\n\nfunction latin1Write (buf, string, offset, length) {\n  return asciiWrite(buf, string, offset, length)\n}\n\nfunction base64Write (buf, string, offset, length) {\n  return blitBuffer(base64ToBytes(string), buf, offset, length)\n}\n\nfunction ucs2Write (buf, string, offset, length) {\n  return blitBuffer(utf16leToBytes(string, buf.length - offset), buf, offset, length)\n}\n\nBuffer.prototype.write = function write (string, offset, length, encoding) {\n  // Buffer#write(string)\n  if (offset === undefined) {\n    encoding = 'utf8'\n    length = this.length\n    offset = 0\n  // Buffer#write(string, encoding)\n  } else if (length === undefined && typeof offset === 'string') {\n    encoding = offset\n    length = this.length\n    offset = 0\n  // Buffer#write(string, offset[, length][, encoding])\n  } else if (isFinite(offset)) {\n    offset = offset | 0\n    if (isFinite(length)) {\n      length = length | 0\n      if (encoding === undefined) encoding = 'utf8'\n    } else {\n      encoding = length\n      length = undefined\n    }\n  // legacy write(string, encoding, offset, length) - remove in v0.13\n  } else {\n    throw new Error(\n      'Buffer.write(string, encoding, offset[, length]) is no longer supported'\n    )\n  }\n\n  var remaining = this.length - offset\n  if (length === undefined || length > remaining) length = remaining\n\n  if ((string.length > 0 && (length < 0 || offset < 0)) || offset > this.length) {\n    throw new RangeError('Attempt to write outside buffer bounds')\n  }\n\n  if (!encoding) encoding = 'utf8'\n\n  var loweredCase = false\n  for (;;) {\n    switch (encoding) {\n      case 'hex':\n        return hexWrite(this, string, offset, length)\n\n      case 'utf8':\n      case 'utf-8':\n        return utf8Write(this, string, offset, length)\n\n      case 'ascii':\n        return asciiWrite(this, string, offset, length)\n\n      case 'latin1':\n      case 'binary':\n        return latin1Write(this, string, offset, length)\n\n      case 'base64':\n        // Warning: maxLength not taken into account in base64Write\n        return base64Write(this, string, offset, length)\n\n      case 'ucs2':\n      case 'ucs-2':\n      case 'utf16le':\n      case 'utf-16le':\n        return ucs2Write(this, string, offset, length)\n\n      default:\n        if (loweredCase) throw new TypeError('Unknown encoding: ' + encoding)\n        encoding = ('' + encoding).toLowerCase()\n        loweredCase = true\n    }\n  }\n}\n\nBuffer.prototype.toJSON = function toJSON () {\n  return {\n    type: 'Buffer',\n    data: Array.prototype.slice.call(this._arr || this, 0)\n  }\n}\n\nfunction base64Slice (buf, start, end) {\n  if (start === 0 && end === buf.length) {\n    return base64.fromByteArray(buf)\n  } else {\n    return base64.fromByteArray(buf.slice(start, end))\n  }\n}\n\nfunction utf8Slice (buf, start, end) {\n  end = Math.min(buf.length, end)\n  var res = []\n\n  var i = start\n  while (i < end) {\n    var firstByte = buf[i]\n    var codePoint = null\n    var bytesPerSequence = (firstByte > 0xEF) ? 4\n      : (firstByte > 0xDF) ? 3\n      : (firstByte > 0xBF) ? 2\n      : 1\n\n    if (i + bytesPerSequence <= end) {\n      var secondByte, thirdByte, fourthByte, tempCodePoint\n\n      switch (bytesPerSequence) {\n        case 1:\n          if (firstByte < 0x80) {\n            codePoint = firstByte\n          }\n          break\n        case 2:\n          secondByte = buf[i + 1]\n          if ((secondByte & 0xC0) === 0x80) {\n            tempCodePoint = (firstByte & 0x1F) << 0x6 | (secondByte & 0x3F)\n            if (tempCodePoint > 0x7F) {\n              codePoint = tempCodePoint\n            }\n          }\n          break\n        case 3:\n          secondByte = buf[i + 1]\n          thirdByte = buf[i + 2]\n          if ((secondByte & 0xC0) === 0x80 && (thirdByte & 0xC0) === 0x80) {\n            tempCodePoint = (firstByte & 0xF) << 0xC | (secondByte & 0x3F) << 0x6 | (thirdByte & 0x3F)\n            if (tempCodePoint > 0x7FF && (tempCodePoint < 0xD800 || tempCodePoint > 0xDFFF)) {\n              codePoint = tempCodePoint\n            }\n          }\n          break\n        case 4:\n          secondByte = buf[i + 1]\n          thirdByte = buf[i + 2]\n          fourthByte = buf[i + 3]\n          if ((secondByte & 0xC0) === 0x80 && (thirdByte & 0xC0) === 0x80 && (fourthByte & 0xC0) === 0x80) {\n            tempCodePoint = (firstByte & 0xF) << 0x12 | (secondByte & 0x3F) << 0xC | (thirdByte & 0x3F) << 0x6 | (fourthByte & 0x3F)\n            if (tempCodePoint > 0xFFFF && tempCodePoint < 0x110000) {\n              codePoint = tempCodePoint\n            }\n          }\n      }\n    }\n\n    if (codePoint === null) {\n      // we did not generate a valid codePoint so insert a\n      // replacement char (U+FFFD) and advance only 1 byte\n      codePoint = 0xFFFD\n      bytesPerSequence = 1\n    } else if (codePoint > 0xFFFF) {\n      // encode to utf16 (surrogate pair dance)\n      codePoint -= 0x10000\n      res.push(codePoint >>> 10 & 0x3FF | 0xD800)\n      codePoint = 0xDC00 | codePoint & 0x3FF\n    }\n\n    res.push(codePoint)\n    i += bytesPerSequence\n  }\n\n  return decodeCodePointsArray(res)\n}\n\n// Based on http://stackoverflow.com/a/22747272/680742, the browser with\n// the lowest limit is Chrome, with 0x10000 args.\n// We go 1 magnitude less, for safety\nvar MAX_ARGUMENTS_LENGTH = 0x1000\n\nfunction decodeCodePointsArray (codePoints) {\n  var len = codePoints.length\n  if (len <= MAX_ARGUMENTS_LENGTH) {\n    return String.fromCharCode.apply(String, codePoints) // avoid extra slice()\n  }\n\n  // Decode in chunks to avoid \"call stack size exceeded\".\n  var res = ''\n  var i = 0\n  while (i < len) {\n    res += String.fromCharCode.apply(\n      String,\n      codePoints.slice(i, i += MAX_ARGUMENTS_LENGTH)\n    )\n  }\n  return res\n}\n\nfunction asciiSlice (buf, start, end) {\n  var ret = ''\n  end = Math.min(buf.length, end)\n\n  for (var i = start; i < end; ++i) {\n    ret += String.fromCharCode(buf[i] & 0x7F)\n  }\n  return ret\n}\n\nfunction latin1Slice (buf, start, end) {\n  var ret = ''\n  end = Math.min(buf.length, end)\n\n  for (var i = start; i < end; ++i) {\n    ret += String.fromCharCode(buf[i])\n  }\n  return ret\n}\n\nfunction hexSlice (buf, start, end) {\n  var len = buf.length\n\n  if (!start || start < 0) start = 0\n  if (!end || end < 0 || end > len) end = len\n\n  var out = ''\n  for (var i = start; i < end; ++i) {\n    out += toHex(buf[i])\n  }\n  return out\n}\n\nfunction utf16leSlice (buf, start, end) {\n  var bytes = buf.slice(start, end)\n  var res = ''\n  for (var i = 0; i < bytes.length; i += 2) {\n    res += String.fromCharCode(bytes[i] + bytes[i + 1] * 256)\n  }\n  return res\n}\n\nBuffer.prototype.slice = function slice (start, end) {\n  var len = this.length\n  start = ~~start\n  end = end === undefined ? len : ~~end\n\n  if (start < 0) {\n    start += len\n    if (start < 0) start = 0\n  } else if (start > len) {\n    start = len\n  }\n\n  if (end < 0) {\n    end += len\n    if (end < 0) end = 0\n  } else if (end > len) {\n    end = len\n  }\n\n  if (end < start) end = start\n\n  var newBuf\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    newBuf = this.subarray(start, end)\n    newBuf.__proto__ = Buffer.prototype\n  } else {\n    var sliceLen = end - start\n    newBuf = new Buffer(sliceLen, undefined)\n    for (var i = 0; i < sliceLen; ++i) {\n      newBuf[i] = this[i + start]\n    }\n  }\n\n  return newBuf\n}\n\n/*\n * Need to make sure that buffer isn't trying to write out of bounds.\n */\nfunction checkOffset (offset, ext, length) {\n  if ((offset % 1) !== 0 || offset < 0) throw new RangeError('offset is not uint')\n  if (offset + ext > length) throw new RangeError('Trying to access beyond buffer length')\n}\n\nBuffer.prototype.readUIntLE = function readUIntLE (offset, byteLength, noAssert) {\n  offset = offset | 0\n  byteLength = byteLength | 0\n  if (!noAssert) checkOffset(offset, byteLength, this.length)\n\n  var val = this[offset]\n  var mul = 1\n  var i = 0\n  while (++i < byteLength && (mul *= 0x100)) {\n    val += this[offset + i] * mul\n  }\n\n  return val\n}\n\nBuffer.prototype.readUIntBE = function readUIntBE (offset, byteLength, noAssert) {\n  offset = offset | 0\n  byteLength = byteLength | 0\n  if (!noAssert) {\n    checkOffset(offset, byteLength, this.length)\n  }\n\n  var val = this[offset + --byteLength]\n  var mul = 1\n  while (byteLength > 0 && (mul *= 0x100)) {\n    val += this[offset + --byteLength] * mul\n  }\n\n  return val\n}\n\nBuffer.prototype.readUInt8 = function readUInt8 (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 1, this.length)\n  return this[offset]\n}\n\nBuffer.prototype.readUInt16LE = function readUInt16LE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 2, this.length)\n  return this[offset] | (this[offset + 1] << 8)\n}\n\nBuffer.prototype.readUInt16BE = function readUInt16BE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 2, this.length)\n  return (this[offset] << 8) | this[offset + 1]\n}\n\nBuffer.prototype.readUInt32LE = function readUInt32LE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 4, this.length)\n\n  return ((this[offset]) |\n      (this[offset + 1] << 8) |\n      (this[offset + 2] << 16)) +\n      (this[offset + 3] * 0x1000000)\n}\n\nBuffer.prototype.readUInt32BE = function readUInt32BE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 4, this.length)\n\n  return (this[offset] * 0x1000000) +\n    ((this[offset + 1] << 16) |\n    (this[offset + 2] << 8) |\n    this[offset + 3])\n}\n\nBuffer.prototype.readIntLE = function readIntLE (offset, byteLength, noAssert) {\n  offset = offset | 0\n  byteLength = byteLength | 0\n  if (!noAssert) checkOffset(offset, byteLength, this.length)\n\n  var val = this[offset]\n  var mul = 1\n  var i = 0\n  while (++i < byteLength && (mul *= 0x100)) {\n    val += this[offset + i] * mul\n  }\n  mul *= 0x80\n\n  if (val >= mul) val -= Math.pow(2, 8 * byteLength)\n\n  return val\n}\n\nBuffer.prototype.readIntBE = function readIntBE (offset, byteLength, noAssert) {\n  offset = offset | 0\n  byteLength = byteLength | 0\n  if (!noAssert) checkOffset(offset, byteLength, this.length)\n\n  var i = byteLength\n  var mul = 1\n  var val = this[offset + --i]\n  while (i > 0 && (mul *= 0x100)) {\n    val += this[offset + --i] * mul\n  }\n  mul *= 0x80\n\n  if (val >= mul) val -= Math.pow(2, 8 * byteLength)\n\n  return val\n}\n\nBuffer.prototype.readInt8 = function readInt8 (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 1, this.length)\n  if (!(this[offset] & 0x80)) return (this[offset])\n  return ((0xff - this[offset] + 1) * -1)\n}\n\nBuffer.prototype.readInt16LE = function readInt16LE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 2, this.length)\n  var val = this[offset] | (this[offset + 1] << 8)\n  return (val & 0x8000) ? val | 0xFFFF0000 : val\n}\n\nBuffer.prototype.readInt16BE = function readInt16BE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 2, this.length)\n  var val = this[offset + 1] | (this[offset] << 8)\n  return (val & 0x8000) ? val | 0xFFFF0000 : val\n}\n\nBuffer.prototype.readInt32LE = function readInt32LE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 4, this.length)\n\n  return (this[offset]) |\n    (this[offset + 1] << 8) |\n    (this[offset + 2] << 16) |\n    (this[offset + 3] << 24)\n}\n\nBuffer.prototype.readInt32BE = function readInt32BE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 4, this.length)\n\n  return (this[offset] << 24) |\n    (this[offset + 1] << 16) |\n    (this[offset + 2] << 8) |\n    (this[offset + 3])\n}\n\nBuffer.prototype.readFloatLE = function readFloatLE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 4, this.length)\n  return ieee754.read(this, offset, true, 23, 4)\n}\n\nBuffer.prototype.readFloatBE = function readFloatBE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 4, this.length)\n  return ieee754.read(this, offset, false, 23, 4)\n}\n\nBuffer.prototype.readDoubleLE = function readDoubleLE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 8, this.length)\n  return ieee754.read(this, offset, true, 52, 8)\n}\n\nBuffer.prototype.readDoubleBE = function readDoubleBE (offset, noAssert) {\n  if (!noAssert) checkOffset(offset, 8, this.length)\n  return ieee754.read(this, offset, false, 52, 8)\n}\n\nfunction checkInt (buf, value, offset, ext, max, min) {\n  if (!Buffer.isBuffer(buf)) throw new TypeError('\"buffer\" argument must be a Buffer instance')\n  if (value > max || value < min) throw new RangeError('\"value\" argument is out of bounds')\n  if (offset + ext > buf.length) throw new RangeError('Index out of range')\n}\n\nBuffer.prototype.writeUIntLE = function writeUIntLE (value, offset, byteLength, noAssert) {\n  value = +value\n  offset = offset | 0\n  byteLength = byteLength | 0\n  if (!noAssert) {\n    var maxBytes = Math.pow(2, 8 * byteLength) - 1\n    checkInt(this, value, offset, byteLength, maxBytes, 0)\n  }\n\n  var mul = 1\n  var i = 0\n  this[offset] = value & 0xFF\n  while (++i < byteLength && (mul *= 0x100)) {\n    this[offset + i] = (value / mul) & 0xFF\n  }\n\n  return offset + byteLength\n}\n\nBuffer.prototype.writeUIntBE = function writeUIntBE (value, offset, byteLength, noAssert) {\n  value = +value\n  offset = offset | 0\n  byteLength = byteLength | 0\n  if (!noAssert) {\n    var maxBytes = Math.pow(2, 8 * byteLength) - 1\n    checkInt(this, value, offset, byteLength, maxBytes, 0)\n  }\n\n  var i = byteLength - 1\n  var mul = 1\n  this[offset + i] = value & 0xFF\n  while (--i >= 0 && (mul *= 0x100)) {\n    this[offset + i] = (value / mul) & 0xFF\n  }\n\n  return offset + byteLength\n}\n\nBuffer.prototype.writeUInt8 = function writeUInt8 (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 1, 0xff, 0)\n  if (!Buffer.TYPED_ARRAY_SUPPORT) value = Math.floor(value)\n  this[offset] = (value & 0xff)\n  return offset + 1\n}\n\nfunction objectWriteUInt16 (buf, value, offset, littleEndian) {\n  if (value < 0) value = 0xffff + value + 1\n  for (var i = 0, j = Math.min(buf.length - offset, 2); i < j; ++i) {\n    buf[offset + i] = (value & (0xff << (8 * (littleEndian ? i : 1 - i)))) >>>\n      (littleEndian ? i : 1 - i) * 8\n  }\n}\n\nBuffer.prototype.writeUInt16LE = function writeUInt16LE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 2, 0xffff, 0)\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset] = (value & 0xff)\n    this[offset + 1] = (value >>> 8)\n  } else {\n    objectWriteUInt16(this, value, offset, true)\n  }\n  return offset + 2\n}\n\nBuffer.prototype.writeUInt16BE = function writeUInt16BE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 2, 0xffff, 0)\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset] = (value >>> 8)\n    this[offset + 1] = (value & 0xff)\n  } else {\n    objectWriteUInt16(this, value, offset, false)\n  }\n  return offset + 2\n}\n\nfunction objectWriteUInt32 (buf, value, offset, littleEndian) {\n  if (value < 0) value = 0xffffffff + value + 1\n  for (var i = 0, j = Math.min(buf.length - offset, 4); i < j; ++i) {\n    buf[offset + i] = (value >>> (littleEndian ? i : 3 - i) * 8) & 0xff\n  }\n}\n\nBuffer.prototype.writeUInt32LE = function writeUInt32LE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 4, 0xffffffff, 0)\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset + 3] = (value >>> 24)\n    this[offset + 2] = (value >>> 16)\n    this[offset + 1] = (value >>> 8)\n    this[offset] = (value & 0xff)\n  } else {\n    objectWriteUInt32(this, value, offset, true)\n  }\n  return offset + 4\n}\n\nBuffer.prototype.writeUInt32BE = function writeUInt32BE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 4, 0xffffffff, 0)\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset] = (value >>> 24)\n    this[offset + 1] = (value >>> 16)\n    this[offset + 2] = (value >>> 8)\n    this[offset + 3] = (value & 0xff)\n  } else {\n    objectWriteUInt32(this, value, offset, false)\n  }\n  return offset + 4\n}\n\nBuffer.prototype.writeIntLE = function writeIntLE (value, offset, byteLength, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) {\n    var limit = Math.pow(2, 8 * byteLength - 1)\n\n    checkInt(this, value, offset, byteLength, limit - 1, -limit)\n  }\n\n  var i = 0\n  var mul = 1\n  var sub = 0\n  this[offset] = value & 0xFF\n  while (++i < byteLength && (mul *= 0x100)) {\n    if (value < 0 && sub === 0 && this[offset + i - 1] !== 0) {\n      sub = 1\n    }\n    this[offset + i] = ((value / mul) >> 0) - sub & 0xFF\n  }\n\n  return offset + byteLength\n}\n\nBuffer.prototype.writeIntBE = function writeIntBE (value, offset, byteLength, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) {\n    var limit = Math.pow(2, 8 * byteLength - 1)\n\n    checkInt(this, value, offset, byteLength, limit - 1, -limit)\n  }\n\n  var i = byteLength - 1\n  var mul = 1\n  var sub = 0\n  this[offset + i] = value & 0xFF\n  while (--i >= 0 && (mul *= 0x100)) {\n    if (value < 0 && sub === 0 && this[offset + i + 1] !== 0) {\n      sub = 1\n    }\n    this[offset + i] = ((value / mul) >> 0) - sub & 0xFF\n  }\n\n  return offset + byteLength\n}\n\nBuffer.prototype.writeInt8 = function writeInt8 (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 1, 0x7f, -0x80)\n  if (!Buffer.TYPED_ARRAY_SUPPORT) value = Math.floor(value)\n  if (value < 0) value = 0xff + value + 1\n  this[offset] = (value & 0xff)\n  return offset + 1\n}\n\nBuffer.prototype.writeInt16LE = function writeInt16LE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 2, 0x7fff, -0x8000)\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset] = (value & 0xff)\n    this[offset + 1] = (value >>> 8)\n  } else {\n    objectWriteUInt16(this, value, offset, true)\n  }\n  return offset + 2\n}\n\nBuffer.prototype.writeInt16BE = function writeInt16BE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 2, 0x7fff, -0x8000)\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset] = (value >>> 8)\n    this[offset + 1] = (value & 0xff)\n  } else {\n    objectWriteUInt16(this, value, offset, false)\n  }\n  return offset + 2\n}\n\nBuffer.prototype.writeInt32LE = function writeInt32LE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 4, 0x7fffffff, -0x80000000)\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset] = (value & 0xff)\n    this[offset + 1] = (value >>> 8)\n    this[offset + 2] = (value >>> 16)\n    this[offset + 3] = (value >>> 24)\n  } else {\n    objectWriteUInt32(this, value, offset, true)\n  }\n  return offset + 4\n}\n\nBuffer.prototype.writeInt32BE = function writeInt32BE (value, offset, noAssert) {\n  value = +value\n  offset = offset | 0\n  if (!noAssert) checkInt(this, value, offset, 4, 0x7fffffff, -0x80000000)\n  if (value < 0) value = 0xffffffff + value + 1\n  if (Buffer.TYPED_ARRAY_SUPPORT) {\n    this[offset] = (value >>> 24)\n    this[offset + 1] = (value >>> 16)\n    this[offset + 2] = (value >>> 8)\n    this[offset + 3] = (value & 0xff)\n  } else {\n    objectWriteUInt32(this, value, offset, false)\n  }\n  return offset + 4\n}\n\nfunction checkIEEE754 (buf, value, offset, ext, max, min) {\n  if (offset + ext > buf.length) throw new RangeError('Index out of range')\n  if (offset < 0) throw new RangeError('Index out of range')\n}\n\nfunction writeFloat (buf, value, offset, littleEndian, noAssert) {\n  if (!noAssert) {\n    checkIEEE754(buf, value, offset, 4, 3.4028234663852886e+38, -3.4028234663852886e+38)\n  }\n  ieee754.write(buf, value, offset, littleEndian, 23, 4)\n  return offset + 4\n}\n\nBuffer.prototype.writeFloatLE = function writeFloatLE (value, offset, noAssert) {\n  return writeFloat(this, value, offset, true, noAssert)\n}\n\nBuffer.prototype.writeFloatBE = function writeFloatBE (value, offset, noAssert) {\n  return writeFloat(this, value, offset, false, noAssert)\n}\n\nfunction writeDouble (buf, value, offset, littleEndian, noAssert) {\n  if (!noAssert) {\n    checkIEEE754(buf, value, offset, 8, 1.7976931348623157E+308, -1.7976931348623157E+308)\n  }\n  ieee754.write(buf, value, offset, littleEndian, 52, 8)\n  return offset + 8\n}\n\nBuffer.prototype.writeDoubleLE = function writeDoubleLE (value, offset, noAssert) {\n  return writeDouble(this, value, offset, true, noAssert)\n}\n\nBuffer.prototype.writeDoubleBE = function writeDoubleBE (value, offset, noAssert) {\n  return writeDouble(this, value, offset, false, noAssert)\n}\n\n// copy(targetBuffer, targetStart=0, sourceStart=0, sourceEnd=buffer.length)\nBuffer.prototype.copy = function copy (target, targetStart, start, end) {\n  if (!start) start = 0\n  if (!end && end !== 0) end = this.length\n  if (targetStart >= target.length) targetStart = target.length\n  if (!targetStart) targetStart = 0\n  if (end > 0 && end < start) end = start\n\n  // Copy 0 bytes; we're done\n  if (end === start) return 0\n  if (target.length === 0 || this.length === 0) return 0\n\n  // Fatal error conditions\n  if (targetStart < 0) {\n    throw new RangeError('targetStart out of bounds')\n  }\n  if (start < 0 || start >= this.length) throw new RangeError('sourceStart out of bounds')\n  if (end < 0) throw new RangeError('sourceEnd out of bounds')\n\n  // Are we oob?\n  if (end > this.length) end = this.length\n  if (target.length - targetStart < end - start) {\n    end = target.length - targetStart + start\n  }\n\n  var len = end - start\n  var i\n\n  if (this === target && start < targetStart && targetStart < end) {\n    // descending copy from end\n    for (i = len - 1; i >= 0; --i) {\n      target[i + targetStart] = this[i + start]\n    }\n  } else if (len < 1000 || !Buffer.TYPED_ARRAY_SUPPORT) {\n    // ascending copy from start\n    for (i = 0; i < len; ++i) {\n      target[i + targetStart] = this[i + start]\n    }\n  } else {\n    Uint8Array.prototype.set.call(\n      target,\n      this.subarray(start, start + len),\n      targetStart\n    )\n  }\n\n  return len\n}\n\n// Usage:\n//    buffer.fill(number[, offset[, end]])\n//    buffer.fill(buffer[, offset[, end]])\n//    buffer.fill(string[, offset[, end]][, encoding])\nBuffer.prototype.fill = function fill (val, start, end, encoding) {\n  // Handle string cases:\n  if (typeof val === 'string') {\n    if (typeof start === 'string') {\n      encoding = start\n      start = 0\n      end = this.length\n    } else if (typeof end === 'string') {\n      encoding = end\n      end = this.length\n    }\n    if (val.length === 1) {\n      var code = val.charCodeAt(0)\n      if (code < 256) {\n        val = code\n      }\n    }\n    if (encoding !== undefined && typeof encoding !== 'string') {\n      throw new TypeError('encoding must be a string')\n    }\n    if (typeof encoding === 'string' && !Buffer.isEncoding(encoding)) {\n      throw new TypeError('Unknown encoding: ' + encoding)\n    }\n  } else if (typeof val === 'number') {\n    val = val & 255\n  }\n\n  // Invalid ranges are not set to a default, so can range check early.\n  if (start < 0 || this.length < start || this.length < end) {\n    throw new RangeError('Out of range index')\n  }\n\n  if (end <= start) {\n    return this\n  }\n\n  start = start >>> 0\n  end = end === undefined ? this.length : end >>> 0\n\n  if (!val) val = 0\n\n  var i\n  if (typeof val === 'number') {\n    for (i = start; i < end; ++i) {\n      this[i] = val\n    }\n  } else {\n    var bytes = Buffer.isBuffer(val)\n      ? val\n      : utf8ToBytes(new Buffer(val, encoding).toString())\n    var len = bytes.length\n    for (i = 0; i < end - start; ++i) {\n      this[i + start] = bytes[i % len]\n    }\n  }\n\n  return this\n}\n\n// HELPER FUNCTIONS\n// ================\n\nvar INVALID_BASE64_RE = /[^+\\/0-9A-Za-z-_]/g\n\nfunction base64clean (str) {\n  // Node strips out invalid characters like \\n and \\t from the string, base64-js does not\n  str = stringtrim(str).replace(INVALID_BASE64_RE, '')\n  // Node converts strings with length < 2 to ''\n  if (str.length < 2) return ''\n  // Node allows for non-padded base64 strings (missing trailing ===), base64-js does not\n  while (str.length % 4 !== 0) {\n    str = str + '='\n  }\n  return str\n}\n\nfunction stringtrim (str) {\n  if (str.trim) return str.trim()\n  return str.replace(/^\\s+|\\s+$/g, '')\n}\n\nfunction toHex (n) {\n  if (n < 16) return '0' + n.toString(16)\n  return n.toString(16)\n}\n\nfunction utf8ToBytes (string, units) {\n  units = units || Infinity\n  var codePoint\n  var length = string.length\n  var leadSurrogate = null\n  var bytes = []\n\n  for (var i = 0; i < length; ++i) {\n    codePoint = string.charCodeAt(i)\n\n    // is surrogate component\n    if (codePoint > 0xD7FF && codePoint < 0xE000) {\n      // last char was a lead\n      if (!leadSurrogate) {\n        // no lead yet\n        if (codePoint > 0xDBFF) {\n          // unexpected trail\n          if ((units -= 3) > -1) bytes.push(0xEF, 0xBF, 0xBD)\n          continue\n        } else if (i + 1 === length) {\n          // unpaired lead\n          if ((units -= 3) > -1) bytes.push(0xEF, 0xBF, 0xBD)\n          continue\n        }\n\n        // valid lead\n        leadSurrogate = codePoint\n\n        continue\n      }\n\n      // 2 leads in a row\n      if (codePoint < 0xDC00) {\n        if ((units -= 3) > -1) bytes.push(0xEF, 0xBF, 0xBD)\n        leadSurrogate = codePoint\n        continue\n      }\n\n      // valid surrogate pair\n      codePoint = (leadSurrogate - 0xD800 << 10 | codePoint - 0xDC00) + 0x10000\n    } else if (leadSurrogate) {\n      // valid bmp char, but last char was a lead\n      if ((units -= 3) > -1) bytes.push(0xEF, 0xBF, 0xBD)\n    }\n\n    leadSurrogate = null\n\n    // encode utf8\n    if (codePoint < 0x80) {\n      if ((units -= 1) < 0) break\n      bytes.push(codePoint)\n    } else if (codePoint < 0x800) {\n      if ((units -= 2) < 0) break\n      bytes.push(\n        codePoint >> 0x6 | 0xC0,\n        codePoint & 0x3F | 0x80\n      )\n    } else if (codePoint < 0x10000) {\n      if ((units -= 3) < 0) break\n      bytes.push(\n        codePoint >> 0xC | 0xE0,\n        codePoint >> 0x6 & 0x3F | 0x80,\n        codePoint & 0x3F | 0x80\n      )\n    } else if (codePoint < 0x110000) {\n      if ((units -= 4) < 0) break\n      bytes.push(\n        codePoint >> 0x12 | 0xF0,\n        codePoint >> 0xC & 0x3F | 0x80,\n        codePoint >> 0x6 & 0x3F | 0x80,\n        codePoint & 0x3F | 0x80\n      )\n    } else {\n      throw new Error('Invalid code point')\n    }\n  }\n\n  return bytes\n}\n\nfunction asciiToBytes (str) {\n  var byteArray = []\n  for (var i = 0; i < str.length; ++i) {\n    // Node's code seems to be doing this and not & 0x7F..\n    byteArray.push(str.charCodeAt(i) & 0xFF)\n  }\n  return byteArray\n}\n\nfunction utf16leToBytes (str, units) {\n  var c, hi, lo\n  var byteArray = []\n  for (var i = 0; i < str.length; ++i) {\n    if ((units -= 2) < 0) break\n\n    c = str.charCodeAt(i)\n    hi = c >> 8\n    lo = c % 256\n    byteArray.push(lo)\n    byteArray.push(hi)\n  }\n\n  return byteArray\n}\n\nfunction base64ToBytes (str) {\n  return base64.toByteArray(base64clean(str))\n}\n\nfunction blitBuffer (src, dst, offset, length) {\n  for (var i = 0; i < length; ++i) {\n    if ((i + offset >= dst.length) || (i >= src.length)) break\n    dst[i + offset] = src[i]\n  }\n  return i\n}\n\nfunction isnan (val) {\n  return val !== val // eslint-disable-line no-self-compare\n}\n","/*\n * Copyright (c) 2017 Digital Bazaar, Inc. All rights reserved.\n */\n'use strict';\n\nconst util = require('./util');\nconst ActiveContextCache = require('./ActiveContextCache');\nconst JsonLdError = require('./JsonLdError');\n\nconst {\n  isArray: _isArray,\n  isObject: _isObject,\n  isString: _isString,\n  isUndefined: _isUndefined\n} = require('./types');\n\nconst {\n  isAbsolute: _isAbsoluteIri,\n  isRelative: _isRelativeIri,\n  prependBase,\n  parse: parseUrl\n} = require('./url');\n\nconst MAX_CONTEXT_URLS = 10;\n\nconst INITIAL_CONTEXT_CACHE = new Map();\nconst INITIAL_CONTEXT_CACHE_MAX_SIZE = 10000;\n\nconst api = {};\nmodule.exports = api;\n\napi.cache = new ActiveContextCache();\n\n/**\n * Processes a local context and returns a new active context.\n *\n * @param activeCtx the current active context.\n * @param localCtx the local context to process.\n * @param options the context processing options.\n *\n * @return the new active context.\n */\napi.process = ({activeCtx, localCtx, options}) => {\n  // normalize local context to an array of @context objects\n  if(_isObject(localCtx) && '@context' in localCtx &&\n    _isArray(localCtx['@context'])) {\n    localCtx = localCtx['@context'];\n  }\n  const ctxs = _isArray(localCtx) ? localCtx : [localCtx];\n\n  // no contexts in array, return current active context w/o changes\n  if(ctxs.length === 0) {\n    return activeCtx;\n  }\n\n  // process each context in order, update active context\n  // on each iteration to ensure proper caching\n  let rval = activeCtx;\n  for(let i = 0; i < ctxs.length; ++i) {\n    let ctx = ctxs[i];\n\n    // reset to initial context\n    if(ctx === null) {\n      rval = activeCtx = api.getInitialContext(options);\n      continue;\n    }\n\n    // dereference @context key if present\n    if(_isObject(ctx) && '@context' in ctx) {\n      ctx = ctx['@context'];\n    }\n\n    // context must be an object by now, all URLs retrieved before this call\n    if(!_isObject(ctx)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; @context must be an object.',\n        'jsonld.SyntaxError', {code: 'invalid local context', context: ctx});\n    }\n\n    // get context from cache if available\n    if(api.cache) {\n      const cached = api.cache.get(activeCtx, ctx);\n      if(cached) {\n        rval = activeCtx = cached;\n        continue;\n      }\n    }\n\n    // update active context and clone new one before updating\n    activeCtx = rval;\n    rval = rval.clone();\n\n    // define context mappings for keys in local context\n    const defined = {};\n\n    // handle @version\n    if('@version' in ctx) {\n      if(ctx['@version'] !== 1.1) {\n        throw new JsonLdError(\n          'Unsupported JSON-LD version: ' + ctx['@version'],\n          'jsonld.UnsupportedVersion',\n          {code: 'invalid @version value', context: ctx});\n      }\n      if(activeCtx.processingMode &&\n        activeCtx.processingMode === 'json-ld-1.0') {\n        throw new JsonLdError(\n          '@version: ' + ctx['@version'] + ' not compatible with ' +\n          activeCtx.processingMode,\n          'jsonld.ProcessingModeConflict',\n          {code: 'processing mode conflict', context: ctx});\n      }\n      rval.processingMode = 'json-ld-1.1';\n      rval['@version'] = ctx['@version'];\n      defined['@version'] = true;\n    }\n\n    // if not set explicitly, set processingMode to \"json-ld-1.0\"\n    rval.processingMode =\n      rval.processingMode || activeCtx.processingMode || 'json-ld-1.0';\n\n    // handle @base\n    if('@base' in ctx) {\n      let base = ctx['@base'];\n\n      if(base === null) {\n        // no action\n      } else if(_isAbsoluteIri(base)) {\n        base = parseUrl(base);\n      } else if(_isRelativeIri(base)) {\n        base = parseUrl(prependBase(activeCtx['@base'].href, base));\n      } else {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; the value of \"@base\" in a ' +\n          '@context must be an absolute IRI, a relative IRI, or null.',\n          'jsonld.SyntaxError', {code: 'invalid base IRI', context: ctx});\n      }\n\n      rval['@base'] = base;\n      defined['@base'] = true;\n    }\n\n    // handle @vocab\n    if('@vocab' in ctx) {\n      const value = ctx['@vocab'];\n      if(value === null) {\n        delete rval['@vocab'];\n      } else if(!_isString(value)) {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; the value of \"@vocab\" in a ' +\n          '@context must be a string or null.',\n          'jsonld.SyntaxError', {code: 'invalid vocab mapping', context: ctx});\n      } else if(!_isAbsoluteIri(value)) {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; the value of \"@vocab\" in a ' +\n          '@context must be an absolute IRI.',\n          'jsonld.SyntaxError', {code: 'invalid vocab mapping', context: ctx});\n      } else {\n        rval['@vocab'] = value;\n      }\n      defined['@vocab'] = true;\n    }\n\n    // handle @language\n    if('@language' in ctx) {\n      const value = ctx['@language'];\n      if(value === null) {\n        delete rval['@language'];\n      } else if(!_isString(value)) {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; the value of \"@language\" in a ' +\n          '@context must be a string or null.',\n          'jsonld.SyntaxError',\n          {code: 'invalid default language', context: ctx});\n      } else {\n        rval['@language'] = value.toLowerCase();\n      }\n      defined['@language'] = true;\n    }\n\n    // process all other keys\n    for(const key in ctx) {\n      api.createTermDefinition(rval, ctx, key, defined);\n    }\n\n    // cache result\n    if(api.cache) {\n      api.cache.set(activeCtx, ctx, rval);\n    }\n  }\n\n  return rval;\n};\n\n/**\n * Creates a term definition during context processing.\n *\n * @param activeCtx the current active context.\n * @param localCtx the local context being processed.\n * @param term the term in the local context to define the mapping for.\n * @param defined a map of defining/defined keys to detect cycles and prevent\n *          double definitions.\n */\napi.createTermDefinition = (activeCtx, localCtx, term, defined) => {\n  if(term in defined) {\n    // term already defined\n    if(defined[term]) {\n      return;\n    }\n    // cycle detected\n    throw new JsonLdError(\n      'Cyclical context definition detected.',\n      'jsonld.CyclicalContext',\n      {code: 'cyclic IRI mapping', context: localCtx, term: term});\n  }\n\n  // now defining term\n  defined[term] = false;\n\n  if(api.isKeyword(term)) {\n    throw new JsonLdError(\n      'Invalid JSON-LD syntax; keywords cannot be overridden.',\n      'jsonld.SyntaxError',\n      {code: 'keyword redefinition', context: localCtx, term: term});\n  }\n\n  if(term === '') {\n    throw new JsonLdError(\n      'Invalid JSON-LD syntax; a term cannot be an empty string.',\n      'jsonld.SyntaxError',\n      {code: 'invalid term definition', context: localCtx});\n  }\n\n  // remove old mapping\n  if(activeCtx.mappings[term]) {\n    delete activeCtx.mappings[term];\n  }\n\n  // get context term value\n  let value = localCtx[term];\n\n  // clear context entry\n  if(value === null || (_isObject(value) && value['@id'] === null)) {\n    activeCtx.mappings[term] = null;\n    defined[term] = true;\n    return;\n  }\n\n  // convert short-hand value to object w/@id\n  let simpleTerm = false;\n  if(_isString(value)) {\n    simpleTerm = true;\n    value = {'@id': value};\n  }\n\n  if(!_isObject(value)) {\n    throw new JsonLdError(\n      'Invalid JSON-LD syntax; @context term values must be ' +\n      'strings or objects.',\n      'jsonld.SyntaxError',\n      {code: 'invalid term definition', context: localCtx});\n  }\n\n  // create new mapping\n  const mapping = activeCtx.mappings[term] = {};\n  mapping.reverse = false;\n\n  // make sure term definition only has expected keywords\n  const validKeys = ['@container', '@id', '@language', '@reverse', '@type'];\n\n  // JSON-LD 1.1 support\n  if(api.processingMode(activeCtx, 1.1)) {\n    validKeys.push('@context', '@nest', '@prefix');\n  }\n\n  for(const kw in value) {\n    if(!validKeys.includes(kw)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; a term definition must not contain ' + kw,\n        'jsonld.SyntaxError',\n        {code: 'invalid term definition', context: localCtx});\n    }\n  }\n\n  // always compute whether term has a colon as an optimization for\n  // _compactIri\n  const colon = term.indexOf(':');\n  mapping._termHasColon = (colon !== -1);\n\n  if('@reverse' in value) {\n    if('@id' in value) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; a @reverse term definition must not ' +\n        'contain @id.', 'jsonld.SyntaxError',\n        {code: 'invalid reverse property', context: localCtx});\n    }\n    if('@nest' in value) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; a @reverse term definition must not ' +\n        'contain @nest.', 'jsonld.SyntaxError',\n        {code: 'invalid reverse property', context: localCtx});\n    }\n    const reverse = value['@reverse'];\n    if(!_isString(reverse)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; a @context @reverse value must be a string.',\n        'jsonld.SyntaxError', {code: 'invalid IRI mapping', context: localCtx});\n    }\n\n    // expand and add @id mapping\n    const id = api.expandIri(\n      activeCtx, reverse, {vocab: true, base: false}, localCtx, defined);\n    if(!_isAbsoluteIri(id)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; a @context @reverse value must be an ' +\n        'absolute IRI or a blank node identifier.',\n        'jsonld.SyntaxError', {code: 'invalid IRI mapping', context: localCtx});\n    }\n    mapping['@id'] = id;\n    mapping.reverse = true;\n  } else if('@id' in value) {\n    let id = value['@id'];\n    if(!_isString(id)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; a @context @id value must be an array ' +\n        'of strings or a string.',\n        'jsonld.SyntaxError', {code: 'invalid IRI mapping', context: localCtx});\n    }\n    if(id !== term) {\n      // expand and add @id mapping\n      id = api.expandIri(\n        activeCtx, id, {vocab: true, base: false}, localCtx, defined);\n      if(!_isAbsoluteIri(id) && !api.isKeyword(id)) {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; a @context @id value must be an ' +\n          'absolute IRI, a blank node identifier, or a keyword.',\n          'jsonld.SyntaxError',\n          {code: 'invalid IRI mapping', context: localCtx});\n      }\n      mapping['@id'] = id;\n      // indicate if this term may be used as a compact IRI prefix\n      mapping._prefix = (!mapping._termHasColon &&\n        id.match(/[:\\/\\?#\\[\\]@]$/) &&\n        (simpleTerm || api.processingMode(activeCtx, 1.0)));\n    }\n  }\n\n  if(!('@id' in mapping)) {\n    // see if the term has a prefix\n    if(mapping._termHasColon) {\n      const prefix = term.substr(0, colon);\n      if(prefix in localCtx) {\n        // define parent prefix\n        api.createTermDefinition(activeCtx, localCtx, prefix, defined);\n      }\n\n      if(activeCtx.mappings[prefix]) {\n        // set @id based on prefix parent\n        const suffix = term.substr(colon + 1);\n        mapping['@id'] = activeCtx.mappings[prefix]['@id'] + suffix;\n      } else {\n        // term is an absolute IRI\n        mapping['@id'] = term;\n      }\n    } else {\n      // non-IRIs *must* define @ids if @vocab is not available\n      if(!('@vocab' in activeCtx)) {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; @context terms must define an @id.',\n          'jsonld.SyntaxError',\n          {code: 'invalid IRI mapping', context: localCtx, term: term});\n      }\n      // prepend vocab to term\n      mapping['@id'] = activeCtx['@vocab'] + term;\n    }\n  }\n\n  // IRI mapping now defined\n  defined[term] = true;\n\n  if('@type' in value) {\n    let type = value['@type'];\n    if(!_isString(type)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; an @context @type values must be a string.',\n        'jsonld.SyntaxError',\n        {code: 'invalid type mapping', context: localCtx});\n    }\n\n    if(type !== '@id' && type !== '@vocab') {\n      // expand @type to full IRI\n      type = api.expandIri(\n        activeCtx, type, {vocab: true, base: false}, localCtx, defined);\n      if(!_isAbsoluteIri(type)) {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; an @context @type value must be an ' +\n          'absolute IRI.',\n          'jsonld.SyntaxError',\n          {code: 'invalid type mapping', context: localCtx});\n      }\n      if(type.indexOf('_:') === 0) {\n        throw new JsonLdError(\n          'Invalid JSON-LD syntax; an @context @type values must be an IRI, ' +\n          'not a blank node identifier.',\n          'jsonld.SyntaxError',\n          {code: 'invalid type mapping', context: localCtx});\n      }\n    }\n\n    // add @type to mapping\n    mapping['@type'] = type;\n  }\n\n  if('@container' in value) {\n    // normalize container to an array form\n    const container = _isString(value['@container']) ?\n      [value['@container']] : (value['@container'] || []);\n    const validContainers = ['@list', '@set', '@index', '@language'];\n    let isValid = true;\n    const hasSet = container.includes('@set');\n\n    // JSON-LD 1.1 support\n    if(api.processingMode(activeCtx, 1.1)) {\n      validContainers.push('@graph', '@id', '@type');\n\n      // check container length\n      if(container.includes('@list')) {\n        if(container.length !== 1) {\n          throw new JsonLdError(\n            'Invalid JSON-LD syntax; @context @container with @list must ' +\n            'have no other values',\n            'jsonld.SyntaxError',\n            {code: 'invalid container mapping', context: localCtx});\n        }\n      } else if(container.includes('@graph')) {\n        if(container.some(key =>\n          key !== '@graph' && key !== '@id' && key !== '@index' &&\n          key !== '@set')) {\n          throw new JsonLdError(\n            'Invalid JSON-LD syntax; @context @container with @graph must ' +\n            'have no other values other than @id, @index, and @set',\n            'jsonld.SyntaxError',\n            {code: 'invalid container mapping', context: localCtx});\n        }\n      } else {\n        // otherwise, container may also include @set\n        isValid &= container.length <= (hasSet ? 2 : 1);\n      }\n    } else {\n      // in JSON-LD 1.0, container must not be an array (it must be a string,\n      // which is one of the validContainers)\n      isValid &= !_isArray(value['@container']);\n\n      // check container length\n      isValid &= container.length <= 1;\n    }\n\n    // check against valid containers\n    isValid &= container.every(c => validContainers.includes(c));\n\n    // @set not allowed with @list\n    isValid &= !(hasSet && container.includes('@list'));\n\n    if(!isValid) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; @context @container value must be ' +\n        'one of the following: ' + validContainers.join(', '),\n        'jsonld.SyntaxError',\n        {code: 'invalid container mapping', context: localCtx});\n    }\n\n    if(mapping.reverse &&\n      !container.every(c => ['@index', '@set'].includes(c))) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; @context @container value for a @reverse ' +\n        'type definition must be @index or @set.', 'jsonld.SyntaxError',\n        {code: 'invalid reverse property', context: localCtx});\n    }\n\n    // add @container to mapping\n    mapping['@container'] = container;\n  }\n\n  // scoped contexts\n  if('@context' in value) {\n    mapping['@context'] = value['@context'];\n  }\n\n  if('@language' in value && !('@type' in value)) {\n    let language = value['@language'];\n    if(language !== null && !_isString(language)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; @context @language value must be ' +\n        'a string or null.', 'jsonld.SyntaxError',\n        {code: 'invalid language mapping', context: localCtx});\n    }\n\n    // add @language to mapping\n    if(language !== null) {\n      language = language.toLowerCase();\n    }\n    mapping['@language'] = language;\n  }\n\n  // term may be used as a prefix\n  if('@prefix' in value) {\n    if(mapping._termHasColon) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; @context @prefix used on a compact IRI term',\n        'jsonld.SyntaxError',\n        {code: 'invalid term definition', context: localCtx});\n    }\n    if(typeof value['@prefix'] === 'boolean') {\n      mapping._prefix = value['@prefix'] === true;\n    } else {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; @context value for @prefix must be boolean',\n        'jsonld.SyntaxError',\n        {code: 'invalid @prefix value', context: localCtx});\n    }\n  }\n\n  if('@nest' in value) {\n    const nest = value['@nest'];\n    if(!_isString(nest) || (nest !== '@nest' && nest.indexOf('@') === 0)) {\n      throw new JsonLdError(\n        'Invalid JSON-LD syntax; @context @nest value must be ' +\n        'a string which is not a keyword other than @nest.',\n        'jsonld.SyntaxError',\n        {code: 'invalid @nest value', context: localCtx});\n    }\n    mapping['@nest'] = nest;\n  }\n\n  // disallow aliasing @context and @preserve\n  const id = mapping['@id'];\n  if(id === '@context' || id === '@preserve') {\n    throw new JsonLdError(\n      'Invalid JSON-LD syntax; @context and @preserve cannot be aliased.',\n      'jsonld.SyntaxError', {code: 'invalid keyword alias', context: localCtx});\n  }\n};\n\n/**\n * Expands a string to a full IRI. The string may be a term, a prefix, a\n * relative IRI, or an absolute IRI. The associated absolute IRI will be\n * returned.\n *\n * @param activeCtx the current active context.\n * @param value the string to expand.\n * @param relativeTo options for how to resolve relative IRIs:\n *          base: true to resolve against the base IRI, false not to.\n *          vocab: true to concatenate after @vocab, false not to.\n * @param localCtx the local context being processed (only given if called\n *          during context processing).\n * @param defined a map for tracking cycles in context definitions (only given\n *          if called during context processing).\n *\n * @return the expanded value.\n */\napi.expandIri = (activeCtx, value, relativeTo, localCtx, defined) => {\n  // already expanded\n  if(value === null || !_isString(value) || api.isKeyword(value)) {\n    return value;\n  }\n\n  // define term dependency if not defined\n  if(localCtx && value in localCtx && defined[value] !== true) {\n    api.createTermDefinition(activeCtx, localCtx, value, defined);\n  }\n\n  relativeTo = relativeTo || {};\n  if(relativeTo.vocab) {\n    const mapping = activeCtx.mappings[value];\n\n    // value is explicitly ignored with a null mapping\n    if(mapping === null) {\n      return null;\n    }\n\n    if(mapping) {\n      // value is a term\n      return mapping['@id'];\n    }\n  }\n\n  // split value into prefix:suffix\n  const colon = value.indexOf(':');\n  if(colon !== -1) {\n    const prefix = value.substr(0, colon);\n    const suffix = value.substr(colon + 1);\n\n    // do not expand blank nodes (prefix of '_') or already-absolute\n    // IRIs (suffix of '//')\n    if(prefix === '_' || suffix.indexOf('//') === 0) {\n      return value;\n    }\n\n    // prefix dependency not defined, define it\n    if(localCtx && prefix in localCtx) {\n      api.createTermDefinition(activeCtx, localCtx, prefix, defined);\n    }\n\n    // use mapping if prefix is defined\n    const mapping = activeCtx.mappings[prefix];\n    if(mapping) {\n      return mapping['@id'] + suffix;\n    }\n\n    // already absolute IRI\n    return value;\n  }\n\n  // prepend vocab\n  if(relativeTo.vocab && '@vocab' in activeCtx) {\n    return activeCtx['@vocab'] + value;\n  }\n\n  // prepend base\n  if(relativeTo.base) {\n    return prependBase(activeCtx['@base'], value);\n  }\n\n  return value;\n};\n\n/**\n * Gets the initial context.\n *\n * @param options the options to use:\n *          [base] the document base IRI.\n *\n * @return the initial context.\n */\napi.getInitialContext = (options) => {\n  const base = parseUrl(options.base || '');\n  const key = JSON.stringify({base, processingMode: options.processingMode});\n  const cached = INITIAL_CONTEXT_CACHE.get(key);\n  if(cached) {\n    return cached;\n  }\n\n  const initialContext = {\n    '@base': base,\n    processingMode: options.processingMode,\n    mappings: {},\n    inverse: null,\n    getInverse: _createInverseContext,\n    clone: _cloneActiveContext\n  };\n  // TODO: consider using LRU cache instead\n  if(INITIAL_CONTEXT_CACHE.size === INITIAL_CONTEXT_CACHE_MAX_SIZE) {\n    // clear whole cache -- assumes scenario where the cache fills means\n    // the cache isn't being used very efficiently anyway\n    INITIAL_CONTEXT_CACHE.clear();\n  }\n  INITIAL_CONTEXT_CACHE.set(key, initialContext);\n  return initialContext;\n\n  /**\n   * Generates an inverse context for use in the compaction algorithm, if\n   * not already generated for the given active context.\n   *\n   * @return the inverse context.\n   */\n  function _createInverseContext() {\n    const activeCtx = this;\n\n    // lazily create inverse\n    if(activeCtx.inverse) {\n      return activeCtx.inverse;\n    }\n    const inverse = activeCtx.inverse = {};\n\n    // variables for building fast CURIE map\n    const fastCurieMap = activeCtx.fastCurieMap = {};\n    const irisToTerms = {};\n\n    // handle default language\n    const defaultLanguage = activeCtx['@language'] || '@none';\n\n    // create term selections for each mapping in the context, ordered by\n    // shortest and then lexicographically least\n    const mappings = activeCtx.mappings;\n    const terms = Object.keys(mappings).sort(util.compareShortestLeast);\n    for(let i = 0; i < terms.length; ++i) {\n      const term = terms[i];\n      const mapping = mappings[term];\n      if(mapping === null) {\n        continue;\n      }\n\n      let container = mapping['@container'] || '@none';\n      container = [].concat(container).sort().join('');\n\n      // iterate over every IRI in the mapping\n      const ids = [].concat(mapping['@id']);\n      for(let ii = 0; ii < ids.length; ++ii) {\n        const iri = ids[ii];\n        let entry = inverse[iri];\n        const isKeyword = api.isKeyword(iri);\n\n        if(!entry) {\n          // initialize entry\n          inverse[iri] = entry = {};\n\n          if(!isKeyword && !mapping._termHasColon) {\n            // init IRI to term map and fast CURIE prefixes\n            irisToTerms[iri] = [term];\n            const fastCurieEntry = {iri: iri, terms: irisToTerms[iri]};\n            if(iri[0] in fastCurieMap) {\n              fastCurieMap[iri[0]].push(fastCurieEntry);\n            } else {\n              fastCurieMap[iri[0]] = [fastCurieEntry];\n            }\n          }\n        } else if(!isKeyword && !mapping._termHasColon) {\n          // add IRI to term match\n          irisToTerms[iri].push(term);\n        }\n\n        // add new entry\n        if(!entry[container]) {\n          entry[container] = {\n            '@language': {},\n            '@type': {},\n            '@any': {}\n          };\n        }\n        entry = entry[container];\n        _addPreferredTerm(term, entry['@any'], '@none');\n\n        if(mapping.reverse) {\n          // term is preferred for values using @reverse\n          _addPreferredTerm(term, entry['@type'], '@reverse');\n        } else if('@type' in mapping) {\n          // term is preferred for values using specific type\n          _addPreferredTerm(term, entry['@type'], mapping['@type']);\n        } else if('@language' in mapping) {\n          // term is preferred for values using specific language\n          const language = mapping['@language'] || '@null';\n          _addPreferredTerm(term, entry['@language'], language);\n        } else {\n          // term is preferred for values w/default language or no type and\n          // no language\n          // add an entry for the default language\n          _addPreferredTerm(term, entry['@language'], defaultLanguage);\n\n          // add entries for no type and no language\n          _addPreferredTerm(term, entry['@type'], '@none');\n          _addPreferredTerm(term, entry['@language'], '@none');\n        }\n      }\n    }\n\n    // build fast CURIE map\n    for(const key in fastCurieMap) {\n      _buildIriMap(fastCurieMap, key, 1);\n    }\n\n    return inverse;\n  }\n\n  /**\n   * Runs a recursive algorithm to build a lookup map for quickly finding\n   * potential CURIEs.\n   *\n   * @param iriMap the map to build.\n   * @param key the current key in the map to work on.\n   * @param idx the index into the IRI to compare.\n   */\n  function _buildIriMap(iriMap, key, idx) {\n    const entries = iriMap[key];\n    const next = iriMap[key] = {};\n\n    let iri;\n    let letter;\n    for(let i = 0; i < entries.length; ++i) {\n      iri = entries[i].iri;\n      if(idx >= iri.length) {\n        letter = '';\n      } else {\n        letter = iri[idx];\n      }\n      if(letter in next) {\n        next[letter].push(entries[i]);\n      } else {\n        next[letter] = [entries[i]];\n      }\n    }\n\n    for(const key in next) {\n      if(key === '') {\n        continue;\n      }\n      _buildIriMap(next, key, idx + 1);\n    }\n  }\n\n  /**\n   * Adds the term for the given entry if not already added.\n   *\n   * @param term the term to add.\n   * @param entry the inverse context typeOrLanguage entry to add to.\n   * @param typeOrLanguageValue the key in the entry to add to.\n   */\n  function _addPreferredTerm(term, entry, typeOrLanguageValue) {\n    if(!(typeOrLanguageValue in entry)) {\n      entry[typeOrLanguageValue] = term;\n    }\n  }\n\n  /**\n   * Clones an active context, creating a child active context.\n   *\n   * @return a clone (child) of the active context.\n   */\n  function _cloneActiveContext() {\n    const child = {};\n    child['@base'] = this['@base'];\n    child.mappings = util.clone(this.mappings);\n    child.clone = this.clone;\n    child.inverse = null;\n    child.getInverse = this.getInverse;\n    if('@language' in this) {\n      child['@language'] = this['@language'];\n    }\n    if('@vocab' in this) {\n      child['@vocab'] = this['@vocab'];\n    }\n    return child;\n  }\n};\n\n/**\n * Gets the value for the given active context key and type, null if none is\n * set.\n *\n * @param ctx the active context.\n * @param key the context key.\n * @param [type] the type of value to get (eg: '@id', '@type'), if not\n *          specified gets the entire entry for a key, null if not found.\n *\n * @return the value.\n */\napi.getContextValue = (ctx, key, type) => {\n  // return null for invalid key\n  if(key === null) {\n    return null;\n  }\n\n  // get specific entry information\n  if(ctx.mappings[key]) {\n    const entry = ctx.mappings[key];\n\n    if(_isUndefined(type)) {\n      // return whole entry\n      return entry;\n    }\n    if(type in entry) {\n      // return entry value for type\n      return entry[type];\n    }\n  }\n\n  // get default language\n  if(type === '@language' && (type in ctx)) {\n    return ctx[type];\n  }\n\n  return null;\n};\n\n/**\n * Retrieves external @context URLs using the given document loader. Every\n * instance of @context in the input that refers to a URL will be replaced\n * with the JSON @context found at that URL.\n *\n * @param input the JSON-LD input with possible contexts.\n * @param options the options to use:\n *          documentLoader(url, [callback(err, remoteDoc)]) the document loader.\n * @param callback(err, input) called once the operation completes.\n */\napi.getAllContexts = async (input, options) => {\n  return _retrieveContextUrls(input, options);\n};\n\n/**\n * Processing Mode check.\n *\n * @param activeCtx the current active context.\n * @param version the string or numeric version to check.\n *\n * @return boolean.\n */\napi.processingMode = (activeCtx, version) => {\n  if(version.toString() >= '1.1') {\n    return activeCtx.processingMode &&\n      activeCtx.processingMode >= 'json-ld-' + version.toString();\n  } else {\n    return !activeCtx.processingMode ||\n      activeCtx.processingMode === 'json-ld-1.0';\n  }\n};\n\n/**\n * Returns whether or not the given value is a keyword.\n *\n * @param v the value to check.\n *\n * @return true if the value is a keyword, false if not.\n */\napi.isKeyword = v => {\n  if(!_isString(v)) {\n    return false;\n  }\n  switch(v) {\n  case '@base':\n  case '@container':\n  case '@context':\n  case '@default':\n  case '@embed':\n  case '@explicit':\n  case '@graph':\n  case '@id':\n  case '@index':\n  case '@language':\n  case '@list':\n  case '@nest':\n  case '@none':\n  case '@omitDefault':\n  case '@prefix':\n  case '@preserve':\n  case '@requireAll':\n  case '@reverse':\n  case '@set':\n  case '@type':\n  case '@value':\n  case '@version':\n  case '@vocab':\n    return true;\n  }\n  return false;\n};\n\nasync function _retrieveContextUrls(input, options) {\n  const documentLoader = util.normalizeDocumentLoader(options.documentLoader);\n\n  // retrieve all @context URLs in input\n  await retrieve(input, new Set(), documentLoader);\n\n  return input;\n\n  // recursive function that will retrieve all @context URLs in documents\n  async function retrieve(doc, cycles, documentLoader) {\n    if(cycles.size > MAX_CONTEXT_URLS) {\n      throw new JsonLdError(\n        'Maximum number of @context URLs exceeded.',\n        'jsonld.ContextUrlError',\n        {code: 'loading remote context failed', max: MAX_CONTEXT_URLS});\n    }\n\n    // find all URLs in the given document\n    const urls = new Map();\n    _findContextUrls(doc, urls, false, options.base);\n\n    // queue all unretrieved URLs\n    const queue = [...urls.keys()].filter(u => urls.get(u) === false);\n\n    // retrieve URLs in queue\n    return Promise.all(queue.map(async url => {\n      // check for context URL cycle\n      if(cycles.has(url)) {\n        throw new JsonLdError(\n          'Cyclical @context URLs detected.',\n          'jsonld.ContextUrlError',\n          {code: 'recursive context inclusion', url});\n      }\n\n      const _cycles = new Set(cycles);\n      _cycles.add(url);\n      let remoteDoc;\n      let ctx;\n\n      try {\n        remoteDoc = await documentLoader(url);\n        ctx = remoteDoc.document || null;\n        // parse string context as JSON\n        if(_isString(ctx)) {\n          ctx = JSON.parse(ctx);\n        }\n      } catch(e) {\n        throw new JsonLdError(\n          'Dereferencing a URL did not result in a valid JSON-LD object. ' +\n          'Possible causes are an inaccessible URL perhaps due to ' +\n          'a same-origin policy (ensure the server uses CORS if you are ' +\n          'using client-side JavaScript), too many redirects, a ' +\n          'non-JSON response, or more than one HTTP Link Header was ' +\n          'provided for a remote context.',\n          'jsonld.InvalidUrl',\n          {code: 'loading remote context failed', url, cause: e});\n      }\n\n      // ensure ctx is an object\n      if(!_isObject(ctx)) {\n        throw new JsonLdError(\n          'Dereferencing a URL did not result in a JSON object. The ' +\n          'response was valid JSON, but it was not a JSON object.',\n          'jsonld.InvalidUrl',\n          {code: 'invalid remote context', url});\n      }\n\n      // use empty context if no @context key is present\n      if(!('@context' in ctx)) {\n        ctx = {'@context': {}};\n      } else {\n        ctx = {'@context': ctx['@context']};\n      }\n\n      // append @context URL to context if given\n      if(remoteDoc.contextUrl) {\n        if(!_isArray(ctx['@context'])) {\n          ctx['@context'] = [ctx['@context']];\n        }\n        ctx['@context'].push(remoteDoc.contextUrl);\n      }\n\n      // recurse\n      await retrieve(ctx, _cycles, documentLoader);\n\n      // store retrieved context w/replaced @context URLs\n      urls.set(url, ctx['@context']);\n\n      // replace all @context URLs in the document\n      _findContextUrls(doc, urls, true, options.base);\n    }));\n  }\n}\n\n/**\n * Finds all @context URLs in the given JSON-LD input.\n *\n * @param input the JSON-LD input.\n * @param urls a map of URLs (url => false/@contexts).\n * @param replace true to replace the URLs in the given input with the\n *           @contexts from the urls map, false not to.\n * @param base the base IRI to use to resolve relative IRIs.\n *\n * @return true if new URLs to retrieve were found, false if not.\n */\nfunction _findContextUrls(input, urls, replace, base) {\n  if(_isArray(input)) {\n    for(const element of input) {\n      _findContextUrls(element, urls, replace, base);\n    }\n    return;\n  }\n\n  if(!_isObject(input)) {\n    // no @context URLs can be found in non-object input\n    return;\n  }\n\n  // input is an object\n  for(const key in input) {\n    if(key !== '@context') {\n      _findContextUrls(input[key], urls, replace, base);\n      continue;\n    }\n\n    // get @context\n    const ctx = input[key];\n\n    if(_isArray(ctx)) {\n      // array @context\n      let length = ctx.length;\n      for(let i = 0; i < length; ++i) {\n        const _ctx = ctx[i];\n        if(_isString(_ctx)) {\n          const prepended = prependBase(base, _ctx);\n          const resolved = urls.get(prepended);\n          // replace w/@context if requested\n          if(replace) {\n            if(_isArray(resolved)) {\n              // add flattened context\n              Array.prototype.splice.apply(ctx, [i, 1].concat(resolved));\n              i += resolved.length - 1;\n              length = ctx.length;\n            } else {\n              ctx[i] = resolved;\n            }\n          } else if(resolved === undefined) {\n            // @context URL found\n            urls.set(prepended, false);\n          }\n        } else {\n          // look for scoped context\n          for(const key in _ctx) {\n            if(_isObject(_ctx[key])) {\n              _findContextUrls(_ctx[key], urls, replace, base);\n            }\n          }\n        }\n      }\n    } else if(_isString(ctx)) {\n      // string @context\n      const prepended = prependBase(base, ctx);\n      const resolved = urls.get(prepended);\n      // replace w/@context if requested\n      if(replace) {\n        if(resolved !== false) {\n          input[key] = resolved;\n        }\n      } else if(resolved === undefined) {\n        // @context URL found\n        urls.set(prepended, false);\n      }\n    } else {\n      // look for scoped context\n      for(const key in ctx) {\n        if(_isObject(ctx[key])) {\n          _findContextUrls(ctx[key], urls, replace, base);\n        }\n      }\n    }\n  }\n}\n","/*! *****************************************************************************\r\nCopyright (c) Microsoft Corporation. All rights reserved.\r\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use\r\nthis file except in compliance with the License. You may obtain a copy of the\r\nLicense at http://www.apache.org/licenses/LICENSE-2.0\r\n\r\nTHIS CODE IS PROVIDED ON AN *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\r\nKIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\r\nWARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\r\nMERCHANTABLITY OR NON-INFRINGEMENT.\r\n\r\nSee the Apache Version 2.0 License for specific language governing permissions\r\nand limitations under the License.\r\n***************************************************************************** */\r\n/* global Reflect, Promise */\r\n\r\nvar extendStatics = function(d, b) {\r\n    extendStatics = Object.setPrototypeOf ||\r\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\r\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\r\n    return extendStatics(d, b);\r\n};\r\n\r\nexport function __extends(d, b) {\r\n    extendStatics(d, b);\r\n    function __() { this.constructor = d; }\r\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\r\n}\r\n\r\nexport var __assign = function() {\r\n    __assign = Object.assign || function __assign(t) {\r\n        for (var s, i = 1, n = arguments.length; i < n; i++) {\r\n            s = arguments[i];\r\n            for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];\r\n        }\r\n        return t;\r\n    }\r\n    return __assign.apply(this, arguments);\r\n}\r\n\r\nexport function __rest(s, e) {\r\n    var t = {};\r\n    for (var p in s) if (Object.prototype.hasOwnProperty.call(s, p) && e.indexOf(p) < 0)\r\n        t[p] = s[p];\r\n    if (s != null && typeof Object.getOwnPropertySymbols === \"function\")\r\n        for (var i = 0, p = Object.getOwnPropertySymbols(s); i < p.length; i++) if (e.indexOf(p[i]) < 0)\r\n            t[p[i]] = s[p[i]];\r\n    return t;\r\n}\r\n\r\nexport function __decorate(decorators, target, key, desc) {\r\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\r\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\r\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\r\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\r\n}\r\n\r\nexport function __param(paramIndex, decorator) {\r\n    return function (target, key) { decorator(target, key, paramIndex); }\r\n}\r\n\r\nexport function __metadata(metadataKey, metadataValue) {\r\n    if (typeof Reflect === \"object\" && typeof Reflect.metadata === \"function\") return Reflect.metadata(metadataKey, metadataValue);\r\n}\r\n\r\nexport function __awaiter(thisArg, _arguments, P, generator) {\r\n    return new (P || (P = Promise))(function (resolve, reject) {\r\n        function fulfilled(value) { try { step(generator.next(value)); } catch (e) { reject(e); } }\r\n        function rejected(value) { try { step(generator[\"throw\"](value)); } catch (e) { reject(e); } }\r\n        function step(result) { result.done ? resolve(result.value) : new P(function (resolve) { resolve(result.value); }).then(fulfilled, rejected); }\r\n        step((generator = generator.apply(thisArg, _arguments || [])).next());\r\n    });\r\n}\r\n\r\nexport function __generator(thisArg, body) {\r\n    var _ = { label: 0, sent: function() { if (t[0] & 1) throw t[1]; return t[1]; }, trys: [], ops: [] }, f, y, t, g;\r\n    return g = { next: verb(0), \"throw\": verb(1), \"return\": verb(2) }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function() { return this; }), g;\r\n    function verb(n) { return function (v) { return step([n, v]); }; }\r\n    function step(op) {\r\n        if (f) throw new TypeError(\"Generator is already executing.\");\r\n        while (_) try {\r\n            if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\r\n            if (y = 0, t) op = [op[0] & 2, t.value];\r\n            switch (op[0]) {\r\n                case 0: case 1: t = op; break;\r\n                case 4: _.label++; return { value: op[1], done: false };\r\n                case 5: _.label++; y = op[1]; op = [0]; continue;\r\n                case 7: op = _.ops.pop(); _.trys.pop(); continue;\r\n                default:\r\n                    if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) { _ = 0; continue; }\r\n                    if (op[0] === 3 && (!t || (op[1] > t[0] && op[1] < t[3]))) { _.label = op[1]; break; }\r\n                    if (op[0] === 6 && _.label < t[1]) { _.label = t[1]; t = op; break; }\r\n                    if (t && _.label < t[2]) { _.label = t[2]; _.ops.push(op); break; }\r\n                    if (t[2]) _.ops.pop();\r\n                    _.trys.pop(); continue;\r\n            }\r\n            op = body.call(thisArg, _);\r\n        } catch (e) { op = [6, e]; y = 0; } finally { f = t = 0; }\r\n        if (op[0] & 5) throw op[1]; return { value: op[0] ? op[1] : void 0, done: true };\r\n    }\r\n}\r\n\r\nexport function __exportStar(m, exports) {\r\n    for (var p in m) if (!exports.hasOwnProperty(p)) exports[p] = m[p];\r\n}\r\n\r\nexport function __values(o) {\r\n    var m = typeof Symbol === \"function\" && o[Symbol.iterator], i = 0;\r\n    if (m) return m.call(o);\r\n    return {\r\n        next: function () {\r\n            if (o && i >= o.length) o = void 0;\r\n            return { value: o && o[i++], done: !o };\r\n        }\r\n    };\r\n}\r\n\r\nexport function __read(o, n) {\r\n    var m = typeof Symbol === \"function\" && o[Symbol.iterator];\r\n    if (!m) return o;\r\n    var i = m.call(o), r, ar = [], e;\r\n    try {\r\n        while ((n === void 0 || n-- > 0) && !(r = i.next()).done) ar.push(r.value);\r\n    }\r\n    catch (error) { e = { error: error }; }\r\n    finally {\r\n        try {\r\n            if (r && !r.done && (m = i[\"return\"])) m.call(i);\r\n        }\r\n        finally { if (e) throw e.error; }\r\n    }\r\n    return ar;\r\n}\r\n\r\nexport function __spread() {\r\n    for (var ar = [], i = 0; i < arguments.length; i++)\r\n        ar = ar.concat(__read(arguments[i]));\r\n    return ar;\r\n}\r\n\r\nexport function __await(v) {\r\n    return this instanceof __await ? (this.v = v, this) : new __await(v);\r\n}\r\n\r\nexport function __asyncGenerator(thisArg, _arguments, generator) {\r\n    if (!Symbol.asyncIterator) throw new TypeError(\"Symbol.asyncIterator is not defined.\");\r\n    var g = generator.apply(thisArg, _arguments || []), i, q = [];\r\n    return i = {}, verb(\"next\"), verb(\"throw\"), verb(\"return\"), i[Symbol.asyncIterator] = function () { return this; }, i;\r\n    function verb(n) { if (g[n]) i[n] = function (v) { return new Promise(function (a, b) { q.push([n, v, a, b]) > 1 || resume(n, v); }); }; }\r\n    function resume(n, v) { try { step(g[n](v)); } catch (e) { settle(q[0][3], e); } }\r\n    function step(r) { r.value instanceof __await ? Promise.resolve(r.value.v).then(fulfill, reject) : settle(q[0][2], r); }\r\n    function fulfill(value) { resume(\"next\", value); }\r\n    function reject(value) { resume(\"throw\", value); }\r\n    function settle(f, v) { if (f(v), q.shift(), q.length) resume(q[0][0], q[0][1]); }\r\n}\r\n\r\nexport function __asyncDelegator(o) {\r\n    var i, p;\r\n    return i = {}, verb(\"next\"), verb(\"throw\", function (e) { throw e; }), verb(\"return\"), i[Symbol.iterator] = function () { return this; }, i;\r\n    function verb(n, f) { i[n] = o[n] ? function (v) { return (p = !p) ? { value: __await(o[n](v)), done: n === \"return\" } : f ? f(v) : v; } : f; }\r\n}\r\n\r\nexport function __asyncValues(o) {\r\n    if (!Symbol.asyncIterator) throw new TypeError(\"Symbol.asyncIterator is not defined.\");\r\n    var m = o[Symbol.asyncIterator], i;\r\n    return m ? m.call(o) : (o = typeof __values === \"function\" ? __values(o) : o[Symbol.iterator](), i = {}, verb(\"next\"), verb(\"throw\"), verb(\"return\"), i[Symbol.asyncIterator] = function () { return this; }, i);\r\n    function verb(n) { i[n] = o[n] && function (v) { return new Promise(function (resolve, reject) { v = o[n](v), settle(resolve, reject, v.done, v.value); }); }; }\r\n    function settle(resolve, reject, d, v) { Promise.resolve(v).then(function(v) { resolve({ value: v, done: d }); }, reject); }\r\n}\r\n\r\nexport function __makeTemplateObject(cooked, raw) {\r\n    if (Object.defineProperty) { Object.defineProperty(cooked, \"raw\", { value: raw }); } else { cooked.raw = raw; }\r\n    return cooked;\r\n};\r\n\r\nexport function __importStar(mod) {\r\n    if (mod && mod.__esModule) return mod;\r\n    var result = {};\r\n    if (mod != null) for (var k in mod) if (Object.hasOwnProperty.call(mod, k)) result[k] = mod[k];\r\n    result.default = mod;\r\n    return result;\r\n}\r\n\r\nexport function __importDefault(mod) {\r\n    return (mod && mod.__esModule) ? mod : { default: mod };\r\n}\r\n","/**\n * A JavaScript implementation of the JSON-LD API.\n *\n * @author Dave Longley\n *\n * @license BSD 3-Clause License\n * Copyright (c) 2011-2017 Digital Bazaar, Inc.\n * All rights reserved.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are met:\n *\n * Redistributions of source code must retain the above copyright notice,\n * this list of conditions and the following disclaimer.\n *\n * Redistributions in binary form must reproduce the above copyright\n * notice, this list of conditions and the following disclaimer in the\n * documentation and/or other materials provided with the distribution.\n *\n * Neither the name of the Digital Bazaar, Inc. nor the names of its\n * contributors may be used to endorse or promote products derived from\n * this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS\n * IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED\n * TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\n * PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED\n * TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR\n * PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF\n * LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING\n * NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\n * SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n(function() {\n\nconst canonize = require('rdf-canonize');\nconst util = require('./util');\nconst IdentifierIssuer = util.IdentifierIssuer;\nconst JsonLdError = require('./JsonLdError');\nconst NQuads = require('./NQuads');\nconst Rdfa = require('./Rdfa');\n\nconst {expand: _expand} = require('./expand');\nconst {flatten: _flatten} = require('./flatten');\nconst {fromRDF: _fromRDF} = require('./fromRdf');\nconst {toRDF: _toRDF} = require('./toRdf');\n\nconst {\n  frameMergedOrDefault: _frameMergedOrDefault\n} = require('./frame');\n\nconst {\n  isArray: _isArray,\n  isObject: _isObject,\n  isString: _isString\n} = require('./types');\n\nconst {\n  isSubjectReference: _isSubjectReference,\n} = require('./graphTypes');\n\nconst {\n  getInitialContext: _getInitialContext,\n  process: _processContext,\n  getAllContexts: _getAllContexts,\n  expandIri: _expandIri\n} = require('./context');\n\nconst {\n  compact: _compact,\n  compactIri: _compactIri,\n  removePreserve: _removePreserve\n} = require('./compact');\n\nconst {\n  createNodeMap: _createNodeMap,\n  createMergedNodeMap: _createMergedNodeMap,\n  mergeNodeMaps: _mergeNodeMaps\n} = require('./nodeMap');\n\n// determine if in-browser or using node.js\nconst _nodejs = (\n  typeof process !== 'undefined' && process.versions && process.versions.node);\nconst _browser = !_nodejs &&\n  (typeof window !== 'undefined' || typeof self !== 'undefined');\n\n// attaches jsonld API to the given object\nconst wrapper = function(jsonld) {\n\n/* Core API */\n\n/**\n * Performs JSON-LD compaction.\n *\n * @param input the JSON-LD input to compact.\n * @param ctx the context to compact with.\n * @param [options] options to use:\n *          [base] the base IRI to use.\n *          [compactArrays] true to compact arrays to single values when\n *            appropriate, false not to (default: true).\n *          [compactToRelative] true to compact IRIs to be relative to document base,\n *            false to keep absolute (default: true)\n *          [graph] true to always output a top-level graph (default: false).\n *          [expandContext] a context to expand with.\n *          [skipExpansion] true to assume the input is expanded and skip\n *            expansion, false not to, defaults to false.\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n *          [expansionMap(info)] a function that can be used to custom map\n *            unmappable values (or to throw an error when they are detected);\n *            if this function returns `undefined` then the default behavior\n *            will be used.\n *          [framing] true if compaction is occuring during a framing operation.\n *          [compactionMap(info)] a function that can be used to custom map\n *            unmappable values (or to throw an error when they are detected);\n *            if this function returns `undefined` then the default behavior\n *            will be used.\n * @param [callback(err, compacted)] called once the operation completes.\n *\n * @return a Promise that resolves to the compacted output.\n */\njsonld.compact = util.callbackify(async function(input, ctx, options) {\n  if(arguments.length < 2) {\n    throw new TypeError('Could not compact, too few arguments.');\n  }\n\n  if(ctx === null) {\n    throw new JsonLdError(\n      'The compaction context must not be null.',\n      'jsonld.CompactError', {code: 'invalid local context'});\n  }\n\n  // nothing to compact\n  if(input === null) {\n    return null;\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    base: _isString(input) ? input : '',\n    compactArrays: true,\n    compactToRelative: true,\n    graph: false,\n    skipExpansion: false,\n    link: false,\n    issuer: new IdentifierIssuer('_:b')\n  });\n  if(options.link) {\n    // force skip expansion when linking, \"link\" is not part of the public\n    // API, it should only be called from framing\n    options.skipExpansion = true;\n  }\n  if(!options.compactToRelative) {\n    delete options.base;\n  }\n\n  // expand input\n  let expanded;\n  if(options.skipExpansion) {\n    expanded = input;\n  } else {\n    expanded = await jsonld.expand(input, options);\n  }\n\n  // process context\n  const activeCtx = await jsonld.processContext(\n    _getInitialContext(options), ctx, options);\n\n  // do compaction\n  let compacted = _compact({\n    activeCtx,\n    element: expanded,\n    options,\n    compactionMap: options.compactionMap\n  });\n\n  // perform clean up\n  if(options.compactArrays && !options.graph && _isArray(compacted)) {\n    if(compacted.length === 1) {\n      // simplify to a single item\n      compacted = compacted[0];\n    } else if(compacted.length === 0) {\n      // simplify to an empty object\n      compacted = {};\n    }\n  } else if(options.graph && _isObject(compacted)) {\n    // always use array if graph option is on\n    compacted = [compacted];\n  }\n\n  // follow @context key\n  if(_isObject(ctx) && '@context' in ctx) {\n    ctx = ctx['@context'];\n  }\n\n  // build output context\n  ctx = util.clone(ctx);\n  if(!_isArray(ctx)) {\n    ctx = [ctx];\n  }\n  // remove empty contexts\n  const tmp = ctx;\n  ctx = [];\n  for(let i = 0; i < tmp.length; ++i) {\n    if(!_isObject(tmp[i]) || Object.keys(tmp[i]).length > 0) {\n      ctx.push(tmp[i]);\n    }\n  }\n\n  // remove array if only one context\n  const hasContext = (ctx.length > 0);\n  if(ctx.length === 1) {\n    ctx = ctx[0];\n  }\n\n  // add context and/or @graph\n  if(_isArray(compacted)) {\n    // use '@graph' keyword\n    const graphAlias = _compactIri({activeCtx, iri: '@graph', relativeTo: {vocab: true}});\n    const graph = compacted;\n    compacted = {};\n    if(hasContext) {\n      compacted['@context'] = ctx;\n    }\n    compacted[graphAlias] = graph;\n  } else if(_isObject(compacted) && hasContext) {\n    // reorder keys so @context is first\n    const graph = compacted;\n    compacted = {'@context': ctx};\n    for(let key in graph) {\n      compacted[key] = graph[key];\n    }\n  }\n\n  if(options.framing) {\n    // get graph alias\n    const graph = _compactIri({activeCtx, iri: '@graph', relativeTo: {vocab: true}});\n    // remove @preserve from results\n    options.link = {};\n    compacted[graph] = _removePreserve(activeCtx, compacted[graph], options);\n  }\n\n  return compacted;\n});\n\n/**\n * Performs JSON-LD expansion.\n *\n * @param input the JSON-LD input to expand.\n * @param [options] the options to use:\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [keepFreeFloatingNodes] true to keep free-floating nodes,\n *            false not to, defaults to false.\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n *          [expansionMap(info)] a function that can be used to custom map\n *            unmappable values (or to throw an error when they are detected);\n *            if this function returns `undefined` then the default behavior\n *            will be used.\n * @param [callback(err, expanded)] called once the operation completes.\n *\n * @return a Promise that resolves to the expanded output.\n */\njsonld.expand = util.callbackify(async function(input, options) {\n  if(arguments.length < 1) {\n    throw new TypeError('Could not expand, too few arguments.');\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    keepFreeFloatingNodes: false\n  });\n  if(options.expansionMap === false) {\n    options.expansionMap = undefined;\n  }\n\n  // build set of objects that may have @contexts to resolve\n  const toResolve = {};\n\n  // build set of contexts to process prior to expansion\n  const contextsToProcess = [];\n\n  // if an `expandContext` has been given ensure it gets resolved\n  if('expandContext' in options) {\n    const expandContext = util.clone(options.expandContext);\n    if(_isObject(expandContext) && '@context' in expandContext) {\n      toResolve.expandContext = expandContext;\n    } else {\n      toResolve.expandContext = {'@context': expandContext};\n    }\n    contextsToProcess.push(toResolve.expandContext);\n  }\n\n  // if input is a string, attempt to dereference remote document\n  let defaultBase;\n  if(!_isString(input)) {\n    // input is not a URL, do not need to retrieve it first\n    toResolve.input = util.clone(input);\n  } else {\n    // load remote doc\n    const remoteDoc = await jsonld.get(input, options);\n    defaultBase = remoteDoc.documentUrl;\n    toResolve.input = remoteDoc.document;\n    if(remoteDoc.contextUrl) {\n      // context included in HTTP link header and must be resolved\n      toResolve.remoteContext = {'@context': remoteDoc.contextUrl};\n      contextsToProcess.push(toResolve.remoteContext);\n    }\n  }\n\n  // set default base\n  if(!('base' in options)) {\n    options.base = defaultBase || '';\n  }\n\n  // get all contexts in `toResolve`\n  await _getAllContexts(toResolve, options);\n\n  // process any additional contexts\n  let activeCtx = _getInitialContext(options);\n  contextsToProcess.forEach(localCtx => {\n    activeCtx = _processContext({activeCtx, localCtx, options});\n  });\n\n  // expand resolved input\n  let expanded = _expand({\n    activeCtx,\n    element: toResolve.input,\n    options,\n    expansionMap: options.expansionMap\n  });\n\n  // optimize away @graph with no other properties\n  if(_isObject(expanded) && ('@graph' in expanded) &&\n    Object.keys(expanded).length === 1) {\n    expanded = expanded['@graph'];\n  } else if(expanded === null) {\n    expanded = [];\n  }\n\n  // normalize to an array\n  if(!_isArray(expanded)) {\n    expanded = [expanded];\n  }\n\n  return expanded;\n});\n\n/**\n * Performs JSON-LD flattening.\n *\n * @param input the JSON-LD to flatten.\n * @param ctx the context to use to compact the flattened output, or null.\n * @param [options] the options to use:\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, flattened)] called once the operation completes.\n *\n * @return a Promise that resolves to the flattened output.\n */\njsonld.flatten = util.callbackify(async function(input, ctx, options) {\n  if(arguments.length < 1) {\n    return new TypeError('Could not flatten, too few arguments.');\n  }\n\n  if(typeof ctx === 'function') {\n    ctx = null;\n  } else {\n    ctx = ctx || null;\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    base: _isString(input) ? input : ''\n  });\n\n  // expand input\n  const expanded = await jsonld.expand(input, options);\n\n  // do flattening\n  const flattened = _flatten(expanded);\n\n  if(ctx === null) {\n    // no compaction required\n    return flattened;\n  }\n\n  // compact result (force @graph option to true, skip expansion)\n  options.graph = true;\n  options.skipExpansion = true;\n  const compacted = await jsonld.compact(flattened, ctx, options);\n\n  return compacted;\n});\n\n/**\n * Performs JSON-LD framing.\n *\n * @param input the JSON-LD input to frame.\n * @param frame the JSON-LD frame to use.\n * @param [options] the framing options.\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [embed] default @embed flag: '@last', '@always', '@never', '@link'\n *            (default: '@last').\n *          [explicit] default @explicit flag (default: false).\n *          [requireAll] default @requireAll flag (default: true).\n *          [omitDefault] default @omitDefault flag (default: false).\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, framed)] called once the operation completes.\n *\n * @return a Promise that resolves to the framed output.\n */\njsonld.frame = util.callbackify(async function(input, frame, options) {\n  if(arguments.length < 2) {\n    throw new TypeError('Could not frame, too few arguments.');\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    base: _isString(input) ? input : '',\n    embed: '@last',\n    explicit: false,\n    requireAll: true,\n    omitDefault: false,\n    pruneBlankNodeIdentifiers: true,\n    bnodesToClear: []\n  });\n\n  // if frame is a string, attempt to dereference remote document\n  if(_isString(frame)) {\n    // load remote doc\n    const remoteDoc = await jsonld.get(frame, options);\n    frame = remoteDoc.document;\n\n    if(remoteDoc.contextUrl) {\n      // inject link header @context into frame\n      let ctx = frame['@context'];\n      if(!ctx) {\n        ctx = remoteDoc.contextUrl;\n      } else if(_isArray(ctx)) {\n        ctx.push(remoteDoc.contextUrl);\n      } else {\n        ctx = [ctx, remoteDoc.contextUrl];\n      }\n      frame['@context'] = ctx;\n    }\n  }\n\n  let frameContext = frame ? frame['@context'] || {} : {};\n\n  // expand input\n  const expanded = await jsonld.expand(input, options);\n\n  // expand frame\n  const opts = util.clone(options);\n  opts.isFrame = true;\n  opts.keepFreeFloatingNodes = true;\n  const expandedFrame = await jsonld.expand(frame, opts);\n\n  // if the unexpanded frame includes a key expanding to @graph, frame the default graph, otherwise, the merged graph\n  let framed;\n  // FIXME should look for aliases of @graph\n  opts.merged = !('@graph' in frame);\n  // do framing\n  framed = _frameMergedOrDefault(expanded, expandedFrame, opts);\n\n  // compact result (force @graph option to true, skip expansion,\n  // check for linked embeds)\n  opts.graph = true;\n  opts.skipExpansion = true;\n  opts.link = {};\n  opts.framing = true;\n  const compacted = await jsonld.compact(framed, frameContext, opts);\n\n  return compacted;\n});\n\n/**\n * **Experimental**\n *\n * Links a JSON-LD document's nodes in memory.\n *\n * @param input the JSON-LD document to link.\n * @param [ctx] the JSON-LD context to apply.\n * @param [options] the options to use:\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, linked)] called once the operation completes.\n *\n * @return a Promise that resolves to the linked output.\n */\njsonld.link = util.callbackify(async function(input, ctx, options) {\n  // API matches running frame with a wildcard frame and embed: '@link'\n  // get arguments\n  const frame = {};\n  if(ctx) {\n    frame['@context'] = ctx;\n  }\n  frame['@embed'] = '@link';\n  return jsonld.frame(input, frame, options);\n});\n\n/**\n * Performs RDF dataset normalization on the given input. The input is JSON-LD\n * unless the 'inputFormat' option is used. The output is an RDF dataset\n * unless the 'format' option is used.\n *\n * @param input the input to normalize as JSON-LD or as a format specified by\n *          the 'inputFormat' option.\n * @param [options] the options to use:\n *          [algorithm] the normalization algorithm to use, `URDNA2015` or\n *            `URGNA2012` (default: `URGNA2012`).\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [inputFormat] the format if input is not JSON-LD:\n *            'application/n-quads' for N-Quads.\n *          [format] the format if output is a string:\n *            'application/n-quads' for N-Quads.\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, normalized)] called once the operation completes.\n *\n * @return a Promise that resolves to the normalized output.\n */\njsonld.normalize = jsonld.canonize = util.callbackify(async function(\n  input, options) {\n  if(arguments.length < 1) {\n    throw new TypeError('Could not canonize, too few arguments.');\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    base: _isString(input) ? input : '',\n    algorithm: 'URDNA2015'\n  });\n  if('inputFormat' in options) {\n    if(options.inputFormat !== 'application/n-quads' &&\n      options.inputFormat !== 'application/nquads') {\n      throw new JsonLdError(\n        'Unknown canonicalization input format.',\n        'jsonld.CanonizeError');\n    }\n    // TODO: `await` for async parsers\n    const parsedInput = NQuads.parse(input);\n\n    // do canonicalization\n    return canonize.canonize(parsedInput, options);\n  }\n\n  // convert to RDF dataset then do normalization\n  const opts = util.clone(options);\n  delete opts.format;\n  opts.produceGeneralizedRdf = false;\n  const dataset = await jsonld.toRDF(input, opts);\n\n  // do canonicalization\n  return canonize.canonize(dataset, options);\n});\n\n/**\n * Converts an RDF dataset to JSON-LD.\n *\n * @param dataset a serialized string of RDF in a format specified by the\n *          format option or an RDF dataset to convert.\n * @param [options] the options to use:\n *          [format] the format if dataset param must first be parsed:\n *            'application/n-quads' for N-Quads (default).\n *          [rdfParser] a custom RDF-parser to use to parse the dataset.\n *          [useRdfType] true to use rdf:type, false to use @type\n *            (default: false).\n *          [useNativeTypes] true to convert XSD types into native types\n *            (boolean, integer, double), false not to (default: false).\n * @param [callback(err, output)] called once the operation completes.\n *\n * @return a Promise that resolves to the JSON-LD document.\n */\njsonld.fromRDF = util.callbackify(async function(dataset, options) {\n  if(arguments.length < 1) {\n    throw new TypeError('Could not convert from RDF, too few arguments.');\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    format: _isString(dataset) ? 'application/n-quads' : undefined\n  });\n\n  let {format, rdfParser} = options;\n\n  // handle special format\n  if(format) {\n    // check supported formats\n    rdfParser = rdfParser || _rdfParsers[format];\n    if(!rdfParser) {\n      throw new JsonLdError(\n        'Unknown input format.',\n        'jsonld.UnknownFormat', {format});\n    }\n  } else {\n    // no-op parser, assume dataset already parsed\n    rdfParser = () => dataset;\n  }\n\n  // TODO: call `normalizeAsyncFn` on parser fn\n\n  // rdfParser can be callback, promise-based, or synchronous\n  let parsedDataset;\n  if(rdfParser.length > 1) {\n    // convert callback-based rdf parser to promise-based\n    parsedDataset = new Promise((resolve, reject) => {\n      rdfParser(dataset, (err, dataset) => {\n        if(err) {\n          reject(err);\n        } else {\n          resolve(dataset);\n        }\n      });\n    });\n  } else {\n    parsedDataset = Promise.resolve(rdfParser(dataset));\n  }\n\n  parsedDataset = await parsedDataset;\n\n  // back-compat with old parsers that produced legacy dataset format\n  if(!Array.isArray(parsedDataset)) {\n    parsedDataset = NQuads.legacyDatasetToQuads(parsedDataset);\n  }\n\n  return _fromRDF(parsedDataset, options);\n});\n\n/**\n * Outputs the RDF dataset found in the given JSON-LD object.\n *\n * @param input the JSON-LD input.\n * @param [options] the options to use:\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [format] the format to use to output a string:\n *            'application/n-quads' for N-Quads.\n *          [produceGeneralizedRdf] true to output generalized RDF, false\n *            to produce only standard RDF (default: false).\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, dataset)] called once the operation completes.\n *\n * @return a Promise that resolves to the RDF dataset.\n */\njsonld.toRDF = util.callbackify(async function(input, options) {\n  if(arguments.length < 1) {\n    throw new TypeError('Could not convert to RDF, too few arguments.');\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    base: _isString(input) ? input : ''\n  });\n\n  // TODO: support toRDF custom map?\n\n  // expand input\n  const expanded = await jsonld.expand(input, options);\n\n  // output RDF dataset\n  const dataset = _toRDF(expanded, options);\n  if(options.format) {\n    if(options.format === 'application/n-quads' ||\n      options.format === 'application/nquads') {\n      return await NQuads.serialize(dataset);\n    }\n    throw new JsonLdError(\n      'Unknown output format.',\n      'jsonld.UnknownFormat', {format: options.format});\n  }\n\n  return dataset;\n});\n\n/**\n * **Experimental**\n *\n * Recursively flattens the nodes in the given JSON-LD input into a merged\n * map of node ID => node. All graphs will be merged into the default graph.\n *\n * @param input the JSON-LD input.\n * @param [options] the options to use:\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [issuer] a jsonld.IdentifierIssuer to use to label blank nodes.\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, nodeMap)] called once the operation completes.\n *\n * @return a Promise that resolves to the merged node map.\n */\njsonld.createNodeMap = util.callbackify(async function(input, options) {\n  if(arguments.length < 1) {\n    throw new TypeError('Could not create node map, too few arguments.');\n  }\n\n  // set default options\n  options = _setDefaults(options, {\n    base: _isString(input) ? input : ''\n  });\n\n  // expand input\n  const expanded = await jsonld.expand(input, options);\n\n  return _createMergedNodeMap(expanded, options);\n});\n\n/**\n * **Experimental**\n *\n * Merges two or more JSON-LD documents into a single flattened document.\n *\n * @param docs the JSON-LD documents to merge together.\n * @param ctx the context to use to compact the merged result, or null.\n * @param [options] the options to use:\n *          [base] the base IRI to use.\n *          [expandContext] a context to expand with.\n *          [issuer] a jsonld.IdentifierIssuer to use to label blank nodes.\n *          [mergeNodes] true to merge properties for nodes with the same ID,\n *            false to ignore new properties for nodes with the same ID once\n *            the ID has been defined; note that this may not prevent merging\n *            new properties where a node is in the `object` position\n *            (default: true).\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, merged)] called once the operation completes.\n *\n * @return a Promise that resolves to the merged output.\n */\njsonld.merge = util.callbackify(async function(docs, ctx, options) {\n  if(arguments.length < 1) {\n    throw new TypeError('Could not merge, too few arguments.');\n  }\n  if(!_isArray(docs)) {\n    throw new TypeError('Could not merge, \"docs\" must be an array.');\n  }\n\n  if(typeof ctx === 'function') {\n    ctx = null;\n  } else {\n    ctx = ctx || null;\n  }\n\n  // set default options\n  options = _setDefaults(options, {});\n\n  // expand all documents\n  const expanded = await Promise.all(docs.map(doc => {\n    const opts = Object.assign({}, options);\n    return jsonld.expand(doc, opts);\n  }));\n\n  let mergeNodes = true;\n  if('mergeNodes' in options) {\n    mergeNodes = options.mergeNodes;\n  }\n\n  const issuer = options.issuer || new IdentifierIssuer('_:b');\n  const graphs = {'@default': {}};\n\n  for(let i = 0; i < expanded.length; ++i) {\n    // uniquely relabel blank nodes\n    const doc = util.relabelBlankNodes(expanded[i], {\n      issuer: new IdentifierIssuer('_:b' + i + '-')\n    });\n\n    // add nodes to the shared node map graphs if merging nodes, to a\n    // separate graph set if not\n    const _graphs = (mergeNodes || i === 0) ? graphs : {'@default': {}};\n    _createNodeMap(doc, _graphs, '@default', issuer);\n\n    if(_graphs !== graphs) {\n      // merge document graphs but don't merge existing nodes\n      for(let graphName in _graphs) {\n        const _nodeMap = _graphs[graphName];\n        if(!(graphName in graphs)) {\n          graphs[graphName] = _nodeMap;\n          continue;\n        }\n        const nodeMap = graphs[graphName];\n        for(let key in _nodeMap) {\n          if(!(key in nodeMap)) {\n            nodeMap[key] = _nodeMap[key];\n          }\n        }\n      }\n    }\n  }\n\n  // add all non-default graphs to default graph\n  const defaultGraph = _mergeNodeMaps(graphs);\n\n  // produce flattened output\n  const flattened = [];\n  const keys = Object.keys(defaultGraph).sort();\n  for(let ki = 0; ki < keys.length; ++ki) {\n    const node = defaultGraph[keys[ki]];\n    // only add full subjects to top-level\n    if(!_isSubjectReference(node)) {\n      flattened.push(node);\n    }\n  }\n\n  if(ctx === null) {\n    return flattened;\n  }\n\n  // compact result (force @graph option to true, skip expansion)\n  options.graph = true;\n  options.skipExpansion = true;\n  const compacted = await jsonld.compact(flattened, ctx, options);\n\n  return compacted;\n});\n\n/**\n * The default document loader for external documents. If the environment\n * is node.js, a callback-continuation-style document loader is used; otherwise,\n * a promises-style document loader is used.\n *\n * @param url the URL to load.\n * @param callback(err, remoteDoc) called once the operation completes,\n *          if using a non-promises API.\n *\n * @return a promise, if using a promises API.\n */\nObject.defineProperty(jsonld, 'documentLoader', {\n  get: () => jsonld._documentLoader,\n  set: v => jsonld._documentLoader = util.normalizeDocumentLoader(v)\n});\n// default document loader not implemented\njsonld.documentLoader = async url => {\n  throw new JsonLdError(\n    'Could not retrieve a JSON-LD document from the URL. URL ' +\n    'dereferencing not implemented.', 'jsonld.LoadDocumentError',\n    {code: 'loading document failed', url: url});\n};\n\n/**\n * Deprecated default document loader. Do not use or override.\n */\njsonld.loadDocument = util.callbackify(async function() {\n  return jsonld.documentLoader.apply(null, arguments);\n});\n\n/**\n * Gets a remote JSON-LD document using the default document loader or\n * one given in the passed options.\n *\n * @param url the URL to fetch.\n * @param [options] the options to use:\n *          [documentLoader] the document loader to use.\n * @param [callback(err, remoteDoc)] called once the operation completes.\n *\n * @return a Promise that resolves to the retrieved remote document.\n */\njsonld.get = util.callbackify(async function(url, options) {\n  let load;\n  if(typeof options.documentLoader === 'function') {\n    load = util.normalizeDocumentLoader(options.documentLoader);\n  } else {\n    load = jsonld.documentLoader;\n  }\n\n  const remoteDoc = await load(url);\n\n  // TODO: can this be moved into `normalizeDocumentLoader`?\n  try {\n    if(!remoteDoc.document) {\n      throw new JsonLdError(\n        'No remote document found at the given URL.',\n        'jsonld.NullRemoteDocument');\n    }\n    if(_isString(remoteDoc.document)) {\n      remoteDoc.document = JSON.parse(remoteDoc.document);\n    }\n  } catch(e) {\n    throw new JsonLdError(\n      'Could not retrieve a JSON-LD document from the URL.',\n      'jsonld.LoadDocumentError', {\n        code: 'loading document failed',\n        cause: e,\n        remoteDoc: remoteDoc\n      });\n  }\n\n  return remoteDoc;\n});\n\n/**\n * Processes a local context, resolving any URLs as necessary, and returns a\n * new active context in its callback.\n *\n * @param activeCtx the current active context.\n * @param localCtx the local context to process.\n * @param [options] the options to use:\n *          [documentLoader(url, callback(err, remoteDoc))] the document loader.\n * @param [callback(err, activeCtx)] called once the operation completes.\n *\n * @return a Promise that resolves to the new active context.\n */\njsonld.processContext = util.callbackify(async function(\n  activeCtx, localCtx, options) {\n  // set default options\n  options = _setDefaults(options, {\n    base: ''\n  });\n\n  // return initial context early for null context\n  if(localCtx === null) {\n    return _getInitialContext(options);\n  }\n\n  // get URLs in localCtx\n  localCtx = util.clone(localCtx);\n  if(!(_isObject(localCtx) && '@context' in localCtx)) {\n    localCtx = {'@context': localCtx};\n  }\n  let ctx = await _getAllContexts(localCtx, options);\n\n  return _processContext({activeCtx, localCtx: ctx, options});\n});\n\n// backwards compatibility\njsonld.getContextValue = require('./context').getContextValue;\n\n/**\n * Document loaders.\n */\njsonld.documentLoaders = {};\njsonld.documentLoaders.node = require('./documentLoaders/node');\njsonld.documentLoaders.xhr = require('./documentLoaders/xhr');\n\n/**\n * Assigns the default document loader for external document URLs to a built-in\n * default. Supported types currently include: 'xhr' and 'node'.\n *\n * @param type the type to set.\n * @param [params] the parameters required to use the document loader.\n */\njsonld.useDocumentLoader = function(type) {\n  if(!(type in jsonld.documentLoaders)) {\n    throw new JsonLdError(\n      'Unknown document loader type: \"' + type + '\"',\n      'jsonld.UnknownDocumentLoader',\n      {type: type});\n  }\n\n  // set document loader\n  jsonld.documentLoader = jsonld.documentLoaders[type].apply(\n    jsonld, Array.prototype.slice.call(arguments, 1));\n};\n\n/** Registered RDF dataset parsers hashed by content-type. */\nconst _rdfParsers = {};\n\n/**\n * Registers an RDF dataset parser by content-type, for use with\n * jsonld.fromRDF. An RDF dataset parser will always be given two parameters,\n * a string of input and a callback. An RDF dataset parser can be synchronous\n * or asynchronous.\n *\n * If the parser function returns undefined or null then it will be assumed to\n * be asynchronous w/a continuation-passing style and the callback parameter\n * given to the parser MUST be invoked.\n *\n * If it returns a Promise, then it will be assumed to be asynchronous, but the\n * callback parameter MUST NOT be invoked. It should instead be ignored.\n *\n * If it returns an RDF dataset, it will be assumed to be synchronous and the\n * callback parameter MUST NOT be invoked. It should instead be ignored.\n *\n * @param contentType the content-type for the parser.\n * @param parser(input, callback(err, dataset)) the parser function (takes a\n *          string as a parameter and either returns null/undefined and uses\n *          the given callback, returns a Promise, or returns an RDF dataset).\n */\njsonld.registerRDFParser = function(contentType, parser) {\n  _rdfParsers[contentType] = parser;\n};\n\n/**\n * Unregisters an RDF dataset parser by content-type.\n *\n * @param contentType the content-type for the parser.\n */\njsonld.unregisterRDFParser = function(contentType) {\n  delete _rdfParsers[contentType];\n};\n\n// register the N-Quads RDF parser\njsonld.registerRDFParser('application/n-quads', NQuads.parse);\njsonld.registerRDFParser('application/nquads', NQuads.parse);\n\n// register the RDFa API RDF parser\njsonld.registerRDFParser('rdfa-api', Rdfa.parse);\n\n/* URL API */\njsonld.url = require('./url');\n\n/* Utility API */\njsonld.util = util;\n// backwards compatibility\nObject.assign(jsonld, util);\n\n// reexpose API as jsonld.promises for backwards compatability\njsonld.promises = jsonld;\n\n// backwards compatibility\njsonld.RequestQueue = require('./RequestQueue');\n\n/* WebIDL API */\njsonld.JsonLdProcessor = require('./JsonLdProcessor')(jsonld);\n\n// setup browser global JsonLdProcessor\nif(_browser && typeof global.JsonLdProcessor === 'undefined') {\n  Object.defineProperty(global, 'JsonLdProcessor', {\n    writable: true,\n    enumerable: false,\n    configurable: true,\n    value: jsonld.JsonLdProcessor\n  });\n}\n\n// set platform-specific defaults/APIs\nif(_nodejs) {\n  // use node document loader by default\n  jsonld.useDocumentLoader('node');\n} else if(typeof XMLHttpRequest !== 'undefined') {\n  // use xhr document loader by default\n  jsonld.useDocumentLoader('xhr');\n}\n\nfunction _setDefaults(options, {\n  documentLoader = jsonld.documentLoader,\n  ...defaults\n}) {\n  if(typeof options === 'function') {\n    options = {};\n  }\n  options = options || {};\n  return Object.assign({}, {documentLoader}, defaults, options);\n}\n\n// end of jsonld API `wrapper` factory\nreturn jsonld;\n};\n\n// external APIs:\n\n// used to generate a new jsonld API instance\nconst factory = function() {\n  return wrapper(function() {\n    return factory();\n  });\n};\n\nif(!_nodejs && (typeof define === 'function' && define.amd)) {\n  // export AMD API\n  define([], function() {\n    // now that module is defined, wrap main jsonld API instance\n    wrapper(factory);\n    return factory;\n  });\n} else {\n  // wrap the main jsonld API instance\n  wrapper(factory);\n\n  if(typeof require === 'function' &&\n    typeof module !== 'undefined' && module.exports) {\n    // export CommonJS/nodejs API\n    module.exports = factory;\n  }\n\n  if(_browser) {\n    // export simple browser API\n    if(typeof jsonld === 'undefined') {\n      jsonld = jsonldjs = factory;\n    } else {\n      jsonldjs = factory;\n    }\n  }\n}\n\nreturn factory;\n\n})();\n","\"use strict\";\n/*! @license MIT 2013-2016 Ruben Verborgh, Ghent University - imec */\n/* Single-function HTTP(S) request module for browsers */\n/* Translated from https://github.com/LinkedDataFragments/Client.js/blob/master/lib/browser/Request.js */\nObject.defineProperty(exports, \"__esModule\", { value: true });\nconst events_1 = require(\"events\");\nconst parseLink = require(\"parse-link-header\");\nconst stream_1 = require(\"stream\");\n// Headers we cannot send (see https://www.w3.org/TR/XMLHttpRequest/#the-setrequestheader()-method)\nconst UNSAFE_REQUEST_HEADERS = { 'accept-encoding': true, 'user-agent': true, 'referer': true };\nclass Requester {\n    constructor() {\n        this.negotiatedResources = {};\n    }\n    // Creates an HTTP request with the given settings\n    createRequest(settings) {\n        // PERFORMANCE HACK:\n        // Reduce OPTIONS preflight requests by removing the Accept-Datetime header\n        // on requests for resources that are presumed to have been time-negotiated\n        if (this.negotiatedResources[this.removeQuery(settings.url)]) {\n            delete settings.headers['accept-datetime'];\n        }\n        // Create the actual XMLHttpRequest\n        const request = new XMLHttpRequest();\n        const reqHeaders = settings.headers;\n        request.open(settings.method, settings.url, true);\n        request.timeout = settings.timeout;\n        for (const header in reqHeaders) {\n            if (!(header in UNSAFE_REQUEST_HEADERS) && reqHeaders[header]) {\n                request.setRequestHeader(header, reqHeaders[header]);\n            }\n        }\n        // Create a proxy for the XMLHttpRequest\n        const requestProxy = new events_1.EventEmitter();\n        requestProxy.abort = () => { request.abort(); };\n        // Handle the arrival of a response\n        request.onload = () => {\n            // Convert the response into an iterator\n            const response = new stream_1.Readable();\n            response.push(request.responseText || '');\n            response.push(null);\n            response.statusCode = request.status;\n            response.responseUrl = request.responseURL;\n            // Parse the response headers\n            response.headers = {};\n            const resHeaders = response.headers;\n            const rawHeaders = request.getAllResponseHeaders() || '';\n            const headerMatcher = /^([^:\\n\\r]+):[ \\t]*([^\\r\\n]*)$/mg;\n            let match = headerMatcher.exec(rawHeaders);\n            while (match) {\n                resHeaders[match[1].toLowerCase()] = match[2];\n                match = headerMatcher.exec(rawHeaders);\n            }\n            // Emit the response\n            requestProxy.emit('response', response);\n            // If the resource was time-negotiated, store its queryless URI\n            // to enable the PERFORMANCE HACK explained above\n            if (reqHeaders['accept-datetime'] && resHeaders['memento-datetime']) {\n                const resource = this.removeQuery(resHeaders['content-location'] || settings.url);\n                if (!this.negotiatedResources[resource]) {\n                    // Ensure the resource is not a timegate\n                    const links = resHeaders.link && parseLink(resHeaders.link);\n                    const timegate = this.removeQuery(links && links.timegate && links.timegate.url);\n                    if (resource !== timegate) {\n                        this.negotiatedResources[resource] = true;\n                    }\n                }\n            }\n        };\n        // Report errors and timeouts\n        request.onerror = () => {\n            requestProxy.emit('error', new Error('Error requesting ' + settings.url));\n        };\n        request.ontimeout = () => {\n            requestProxy.emit('error', new Error('Timeout requesting ' + settings.url));\n        };\n        // Execute the request\n        request.send();\n        return requestProxy;\n    }\n    // Removes the query string from a URL\n    removeQuery(url) {\n        return url ? url.replace(/\\?.*$/, '') : '';\n    }\n}\nexports.default = Requester;\n//# sourceMappingURL=Requester-browser.js.map","/*! https://mths.be/punycode v1.4.1 by @mathias */\n;(function(root) {\n\n\t/** Detect free variables */\n\tvar freeExports = typeof exports == 'object' && exports &&\n\t\t!exports.nodeType && exports;\n\tvar freeModule = typeof module == 'object' && module &&\n\t\t!module.nodeType && module;\n\tvar freeGlobal = typeof global == 'object' && global;\n\tif (\n\t\tfreeGlobal.global === freeGlobal ||\n\t\tfreeGlobal.window === freeGlobal ||\n\t\tfreeGlobal.self === freeGlobal\n\t) {\n\t\troot = freeGlobal;\n\t}\n\n\t/**\n\t * The `punycode` object.\n\t * @name punycode\n\t * @type Object\n\t */\n\tvar punycode,\n\n\t/** Highest positive signed 32-bit float value */\n\tmaxInt = 2147483647, // aka. 0x7FFFFFFF or 2^31-1\n\n\t/** Bootstring parameters */\n\tbase = 36,\n\ttMin = 1,\n\ttMax = 26,\n\tskew = 38,\n\tdamp = 700,\n\tinitialBias = 72,\n\tinitialN = 128, // 0x80\n\tdelimiter = '-', // '\\x2D'\n\n\t/** Regular expressions */\n\tregexPunycode = /^xn--/,\n\tregexNonASCII = /[^\\x20-\\x7E]/, // unprintable ASCII chars + non-ASCII chars\n\tregexSeparators = /[\\x2E\\u3002\\uFF0E\\uFF61]/g, // RFC 3490 separators\n\n\t/** Error messages */\n\terrors = {\n\t\t'overflow': 'Overflow: input needs wider integers to process',\n\t\t'not-basic': 'Illegal input >= 0x80 (not a basic code point)',\n\t\t'invalid-input': 'Invalid input'\n\t},\n\n\t/** Convenience shortcuts */\n\tbaseMinusTMin = base - tMin,\n\tfloor = Math.floor,\n\tstringFromCharCode = String.fromCharCode,\n\n\t/** Temporary variable */\n\tkey;\n\n\t/*--------------------------------------------------------------------------*/\n\n\t/**\n\t * A generic error utility function.\n\t * @private\n\t * @param {String} type The error type.\n\t * @returns {Error} Throws a `RangeError` with the applicable error message.\n\t */\n\tfunction error(type) {\n\t\tthrow new RangeError(errors[type]);\n\t}\n\n\t/**\n\t * A generic `Array#map` utility function.\n\t * @private\n\t * @param {Array} array The array to iterate over.\n\t * @param {Function} callback The function that gets called for every array\n\t * item.\n\t * @returns {Array} A new array of values returned by the callback function.\n\t */\n\tfunction map(array, fn) {\n\t\tvar length = array.length;\n\t\tvar result = [];\n\t\twhile (length--) {\n\t\t\tresult[length] = fn(array[length]);\n\t\t}\n\t\treturn result;\n\t}\n\n\t/**\n\t * A simple `Array#map`-like wrapper to work with domain name strings or email\n\t * addresses.\n\t * @private\n\t * @param {String} domain The domain name or email address.\n\t * @param {Function} callback The function that gets called for every\n\t * character.\n\t * @returns {Array} A new string of characters returned by the callback\n\t * function.\n\t */\n\tfunction mapDomain(string, fn) {\n\t\tvar parts = string.split('@');\n\t\tvar result = '';\n\t\tif (parts.length > 1) {\n\t\t\t// In email addresses, only the domain name should be punycoded. Leave\n\t\t\t// the local part (i.e. everything up to `@`) intact.\n\t\t\tresult = parts[0] + '@';\n\t\t\tstring = parts[1];\n\t\t}\n\t\t// Avoid `split(regex)` for IE8 compatibility. See #17.\n\t\tstring = string.replace(regexSeparators, '\\x2E');\n\t\tvar labels = string.split('.');\n\t\tvar encoded = map(labels, fn).join('.');\n\t\treturn result + encoded;\n\t}\n\n\t/**\n\t * Creates an array containing the numeric code points of each Unicode\n\t * character in the string. While JavaScript uses UCS-2 internally,\n\t * this function will convert a pair of surrogate halves (each of which\n\t * UCS-2 exposes as separate characters) into a single code point,\n\t * matching UTF-16.\n\t * @see `punycode.ucs2.encode`\n\t * @see <https://mathiasbynens.be/notes/javascript-encoding>\n\t * @memberOf punycode.ucs2\n\t * @name decode\n\t * @param {String} string The Unicode input string (UCS-2).\n\t * @returns {Array} The new array of code points.\n\t */\n\tfunction ucs2decode(string) {\n\t\tvar output = [],\n\t\t    counter = 0,\n\t\t    length = string.length,\n\t\t    value,\n\t\t    extra;\n\t\twhile (counter < length) {\n\t\t\tvalue = string.charCodeAt(counter++);\n\t\t\tif (value >= 0xD800 && value <= 0xDBFF && counter < length) {\n\t\t\t\t// high surrogate, and there is a next character\n\t\t\t\textra = string.charCodeAt(counter++);\n\t\t\t\tif ((extra & 0xFC00) == 0xDC00) { // low surrogate\n\t\t\t\t\toutput.push(((value & 0x3FF) << 10) + (extra & 0x3FF) + 0x10000);\n\t\t\t\t} else {\n\t\t\t\t\t// unmatched surrogate; only append this code unit, in case the next\n\t\t\t\t\t// code unit is the high surrogate of a surrogate pair\n\t\t\t\t\toutput.push(value);\n\t\t\t\t\tcounter--;\n\t\t\t\t}\n\t\t\t} else {\n\t\t\t\toutput.push(value);\n\t\t\t}\n\t\t}\n\t\treturn output;\n\t}\n\n\t/**\n\t * Creates a string based on an array of numeric code points.\n\t * @see `punycode.ucs2.decode`\n\t * @memberOf punycode.ucs2\n\t * @name encode\n\t * @param {Array} codePoints The array of numeric code points.\n\t * @returns {String} The new Unicode string (UCS-2).\n\t */\n\tfunction ucs2encode(array) {\n\t\treturn map(array, function(value) {\n\t\t\tvar output = '';\n\t\t\tif (value > 0xFFFF) {\n\t\t\t\tvalue -= 0x10000;\n\t\t\t\toutput += stringFromCharCode(value >>> 10 & 0x3FF | 0xD800);\n\t\t\t\tvalue = 0xDC00 | value & 0x3FF;\n\t\t\t}\n\t\t\toutput += stringFromCharCode(value);\n\t\t\treturn output;\n\t\t}).join('');\n\t}\n\n\t/**\n\t * Converts a basic code point into a digit/integer.\n\t * @see `digitToBasic()`\n\t * @private\n\t * @param {Number} codePoint The basic numeric code point value.\n\t * @returns {Number} The numeric value of a basic code point (for use in\n\t * representing integers) in the range `0` to `base - 1`, or `base` if\n\t * the code point does not represent a value.\n\t */\n\tfunction basicToDigit(codePoint) {\n\t\tif (codePoint - 48 < 10) {\n\t\t\treturn codePoint - 22;\n\t\t}\n\t\tif (codePoint - 65 < 26) {\n\t\t\treturn codePoint - 65;\n\t\t}\n\t\tif (codePoint - 97 < 26) {\n\t\t\treturn codePoint - 97;\n\t\t}\n\t\treturn base;\n\t}\n\n\t/**\n\t * Converts a digit/integer into a basic code point.\n\t * @see `basicToDigit()`\n\t * @private\n\t * @param {Number} digit The numeric value of a basic code point.\n\t * @returns {Number} The basic code point whose value (when used for\n\t * representing integers) is `digit`, which needs to be in the range\n\t * `0` to `base - 1`. If `flag` is non-zero, the uppercase form is\n\t * used; else, the lowercase form is used. The behavior is undefined\n\t * if `flag` is non-zero and `digit` has no uppercase form.\n\t */\n\tfunction digitToBasic(digit, flag) {\n\t\t//  0..25 map to ASCII a..z or A..Z\n\t\t// 26..35 map to ASCII 0..9\n\t\treturn digit + 22 + 75 * (digit < 26) - ((flag != 0) << 5);\n\t}\n\n\t/**\n\t * Bias adaptation function as per section 3.4 of RFC 3492.\n\t * https://tools.ietf.org/html/rfc3492#section-3.4\n\t * @private\n\t */\n\tfunction adapt(delta, numPoints, firstTime) {\n\t\tvar k = 0;\n\t\tdelta = firstTime ? floor(delta / damp) : delta >> 1;\n\t\tdelta += floor(delta / numPoints);\n\t\tfor (/* no initialization */; delta > baseMinusTMin * tMax >> 1; k += base) {\n\t\t\tdelta = floor(delta / baseMinusTMin);\n\t\t}\n\t\treturn floor(k + (baseMinusTMin + 1) * delta / (delta + skew));\n\t}\n\n\t/**\n\t * Converts a Punycode string of ASCII-only symbols to a string of Unicode\n\t * symbols.\n\t * @memberOf punycode\n\t * @param {String} input The Punycode string of ASCII-only symbols.\n\t * @returns {String} The resulting string of Unicode symbols.\n\t */\n\tfunction decode(input) {\n\t\t// Don't use UCS-2\n\t\tvar output = [],\n\t\t    inputLength = input.length,\n\t\t    out,\n\t\t    i = 0,\n\t\t    n = initialN,\n\t\t    bias = initialBias,\n\t\t    basic,\n\t\t    j,\n\t\t    index,\n\t\t    oldi,\n\t\t    w,\n\t\t    k,\n\t\t    digit,\n\t\t    t,\n\t\t    /** Cached calculation results */\n\t\t    baseMinusT;\n\n\t\t// Handle the basic code points: let `basic` be the number of input code\n\t\t// points before the last delimiter, or `0` if there is none, then copy\n\t\t// the first basic code points to the output.\n\n\t\tbasic = input.lastIndexOf(delimiter);\n\t\tif (basic < 0) {\n\t\t\tbasic = 0;\n\t\t}\n\n\t\tfor (j = 0; j < basic; ++j) {\n\t\t\t// if it's not a basic code point\n\t\t\tif (input.charCodeAt(j) >= 0x80) {\n\t\t\t\terror('not-basic');\n\t\t\t}\n\t\t\toutput.push(input.charCodeAt(j));\n\t\t}\n\n\t\t// Main decoding loop: start just after the last delimiter if any basic code\n\t\t// points were copied; start at the beginning otherwise.\n\n\t\tfor (index = basic > 0 ? basic + 1 : 0; index < inputLength; /* no final expression */) {\n\n\t\t\t// `index` is the index of the next character to be consumed.\n\t\t\t// Decode a generalized variable-length integer into `delta`,\n\t\t\t// which gets added to `i`. The overflow checking is easier\n\t\t\t// if we increase `i` as we go, then subtract off its starting\n\t\t\t// value at the end to obtain `delta`.\n\t\t\tfor (oldi = i, w = 1, k = base; /* no condition */; k += base) {\n\n\t\t\t\tif (index >= inputLength) {\n\t\t\t\t\terror('invalid-input');\n\t\t\t\t}\n\n\t\t\t\tdigit = basicToDigit(input.charCodeAt(index++));\n\n\t\t\t\tif (digit >= base || digit > floor((maxInt - i) / w)) {\n\t\t\t\t\terror('overflow');\n\t\t\t\t}\n\n\t\t\t\ti += digit * w;\n\t\t\t\tt = k <= bias ? tMin : (k >= bias + tMax ? tMax : k - bias);\n\n\t\t\t\tif (digit < t) {\n\t\t\t\t\tbreak;\n\t\t\t\t}\n\n\t\t\t\tbaseMinusT = base - t;\n\t\t\t\tif (w > floor(maxInt / baseMinusT)) {\n\t\t\t\t\terror('overflow');\n\t\t\t\t}\n\n\t\t\t\tw *= baseMinusT;\n\n\t\t\t}\n\n\t\t\tout = output.length + 1;\n\t\t\tbias = adapt(i - oldi, out, oldi == 0);\n\n\t\t\t// `i` was supposed to wrap around from `out` to `0`,\n\t\t\t// incrementing `n` each time, so we'll fix that now:\n\t\t\tif (floor(i / out) > maxInt - n) {\n\t\t\t\terror('overflow');\n\t\t\t}\n\n\t\t\tn += floor(i / out);\n\t\t\ti %= out;\n\n\t\t\t// Insert `n` at position `i` of the output\n\t\t\toutput.splice(i++, 0, n);\n\n\t\t}\n\n\t\treturn ucs2encode(output);\n\t}\n\n\t/**\n\t * Converts a string of Unicode symbols (e.g. a domain name label) to a\n\t * Punycode string of ASCII-only symbols.\n\t * @memberOf punycode\n\t * @param {String} input The string of Unicode symbols.\n\t * @returns {String} The resulting Punycode string of ASCII-only symbols.\n\t */\n\tfunction encode(input) {\n\t\tvar n,\n\t\t    delta,\n\t\t    handledCPCount,\n\t\t    basicLength,\n\t\t    bias,\n\t\t    j,\n\t\t    m,\n\t\t    q,\n\t\t    k,\n\t\t    t,\n\t\t    currentValue,\n\t\t    output = [],\n\t\t    /** `inputLength` will hold the number of code points in `input`. */\n\t\t    inputLength,\n\t\t    /** Cached calculation results */\n\t\t    handledCPCountPlusOne,\n\t\t    baseMinusT,\n\t\t    qMinusT;\n\n\t\t// Convert the input in UCS-2 to Unicode\n\t\tinput = ucs2decode(input);\n\n\t\t// Cache the length\n\t\tinputLength = input.length;\n\n\t\t// Initialize the state\n\t\tn = initialN;\n\t\tdelta = 0;\n\t\tbias = initialBias;\n\n\t\t// Handle the basic code points\n\t\tfor (j = 0; j < inputLength; ++j) {\n\t\t\tcurrentValue = input[j];\n\t\t\tif (currentValue < 0x80) {\n\t\t\t\toutput.push(stringFromCharCode(currentValue));\n\t\t\t}\n\t\t}\n\n\t\thandledCPCount = basicLength = output.length;\n\n\t\t// `handledCPCount` is the number of code points that have been handled;\n\t\t// `basicLength` is the number of basic code points.\n\n\t\t// Finish the basic string - if it is not empty - with a delimiter\n\t\tif (basicLength) {\n\t\t\toutput.push(delimiter);\n\t\t}\n\n\t\t// Main encoding loop:\n\t\twhile (handledCPCount < inputLength) {\n\n\t\t\t// All non-basic code points < n have been handled already. Find the next\n\t\t\t// larger one:\n\t\t\tfor (m = maxInt, j = 0; j < inputLength; ++j) {\n\t\t\t\tcurrentValue = input[j];\n\t\t\t\tif (currentValue >= n && currentValue < m) {\n\t\t\t\t\tm = currentValue;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t// Increase `delta` enough to advance the decoder's <n,i> state to <m,0>,\n\t\t\t// but guard against overflow\n\t\t\thandledCPCountPlusOne = handledCPCount + 1;\n\t\t\tif (m - n > floor((maxInt - delta) / handledCPCountPlusOne)) {\n\t\t\t\terror('overflow');\n\t\t\t}\n\n\t\t\tdelta += (m - n) * handledCPCountPlusOne;\n\t\t\tn = m;\n\n\t\t\tfor (j = 0; j < inputLength; ++j) {\n\t\t\t\tcurrentValue = input[j];\n\n\t\t\t\tif (currentValue < n && ++delta > maxInt) {\n\t\t\t\t\terror('overflow');\n\t\t\t\t}\n\n\t\t\t\tif (currentValue == n) {\n\t\t\t\t\t// Represent delta as a generalized variable-length integer\n\t\t\t\t\tfor (q = delta, k = base; /* no condition */; k += base) {\n\t\t\t\t\t\tt = k <= bias ? tMin : (k >= bias + tMax ? tMax : k - bias);\n\t\t\t\t\t\tif (q < t) {\n\t\t\t\t\t\t\tbreak;\n\t\t\t\t\t\t}\n\t\t\t\t\t\tqMinusT = q - t;\n\t\t\t\t\t\tbaseMinusT = base - t;\n\t\t\t\t\t\toutput.push(\n\t\t\t\t\t\t\tstringFromCharCode(digitToBasic(t + qMinusT % baseMinusT, 0))\n\t\t\t\t\t\t);\n\t\t\t\t\t\tq = floor(qMinusT / baseMinusT);\n\t\t\t\t\t}\n\n\t\t\t\t\toutput.push(stringFromCharCode(digitToBasic(q, 0)));\n\t\t\t\t\tbias = adapt(delta, handledCPCountPlusOne, handledCPCount == basicLength);\n\t\t\t\t\tdelta = 0;\n\t\t\t\t\t++handledCPCount;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\t++delta;\n\t\t\t++n;\n\n\t\t}\n\t\treturn output.join('');\n\t}\n\n\t/**\n\t * Converts a Punycode string representing a domain name or an email address\n\t * to Unicode. Only the Punycoded parts of the input will be converted, i.e.\n\t * it doesn't matter if you call it on a string that has already been\n\t * converted to Unicode.\n\t * @memberOf punycode\n\t * @param {String} input The Punycoded domain name or email address to\n\t * convert to Unicode.\n\t * @returns {String} The Unicode representation of the given Punycode\n\t * string.\n\t */\n\tfunction toUnicode(input) {\n\t\treturn mapDomain(input, function(string) {\n\t\t\treturn regexPunycode.test(string)\n\t\t\t\t? decode(string.slice(4).toLowerCase())\n\t\t\t\t: string;\n\t\t});\n\t}\n\n\t/**\n\t * Converts a Unicode string representing a domain name or an email address to\n\t * Punycode. Only the non-ASCII parts of the domain name will be converted,\n\t * i.e. it doesn't matter if you call it with a domain that's already in\n\t * ASCII.\n\t * @memberOf punycode\n\t * @param {String} input The domain name or email address to convert, as a\n\t * Unicode string.\n\t * @returns {String} The Punycode representation of the given domain name or\n\t * email address.\n\t */\n\tfunction toASCII(input) {\n\t\treturn mapDomain(input, function(string) {\n\t\t\treturn regexNonASCII.test(string)\n\t\t\t\t? 'xn--' + encode(string)\n\t\t\t\t: string;\n\t\t});\n\t}\n\n\t/*--------------------------------------------------------------------------*/\n\n\t/** Define the public API */\n\tpunycode = {\n\t\t/**\n\t\t * A string representing the current Punycode.js version number.\n\t\t * @memberOf punycode\n\t\t * @type String\n\t\t */\n\t\t'version': '1.4.1',\n\t\t/**\n\t\t * An object of methods to convert from JavaScript's internal character\n\t\t * representation (UCS-2) to Unicode code points, and back.\n\t\t * @see <https://mathiasbynens.be/notes/javascript-encoding>\n\t\t * @memberOf punycode\n\t\t * @type Object\n\t\t */\n\t\t'ucs2': {\n\t\t\t'decode': ucs2decode,\n\t\t\t'encode': ucs2encode\n\t\t},\n\t\t'decode': decode,\n\t\t'encode': encode,\n\t\t'toASCII': toASCII,\n\t\t'toUnicode': toUnicode\n\t};\n\n\t/** Expose `punycode` */\n\t// Some AMD build optimizers, like r.js, check for specific condition patterns\n\t// like the following:\n\tif (\n\t\ttypeof define == 'function' &&\n\t\ttypeof define.amd == 'object' &&\n\t\tdefine.amd\n\t) {\n\t\tdefine('punycode', function() {\n\t\t\treturn punycode;\n\t\t});\n\t} else if (freeExports && freeModule) {\n\t\tif (module.exports == freeExports) {\n\t\t\t// in Node.js, io.js, or RingoJS v0.8.0+\n\t\t\tfreeModule.exports = punycode;\n\t\t} else {\n\t\t\t// in Narwhal or RingoJS v0.7.0-\n\t\t\tfor (key in punycode) {\n\t\t\t\tpunycode.hasOwnProperty(key) && (freeExports[key] = punycode[key]);\n\t\t\t}\n\t\t}\n\t} else {\n\t\t// in Rhino or a web browser\n\t\troot.punycode = punycode;\n\t}\n\n}(this));\n","/*\n * Copyright (c) 2017 Digital Bazaar, Inc. All rights reserved.\n */\n'use strict';\n\nconst JsonLdError = require('./JsonLdError');\n\nconst {\n  isArray: _isArray,\n  isObject: _isObject,\n  isString: _isString\n} = require('./types');\n\nconst {\n  isList: _isList,\n  isValue: _isValue,\n  isGraph: _isGraph,\n  isSimpleGraph: _isSimpleGraph,\n  isSubjectReference: _isSubjectReference\n} = require('./graphTypes');\n\nconst {\n  expandIri: _expandIri,\n  getContextValue: _getContextValue,\n  isKeyword: _isKeyword,\n  process: _processContext\n} = require('./context');\n\nconst {\n  removeBase: _removeBase\n} = require('./url');\n\nconst {\n  addValue: _addValue,\n  compareShortestLeast: _compareShortestLeast\n} = require('./util');\n\nconst api = {};\nmodule.exports = api;\n\n/**\n * Recursively compacts an element using the given active context. All values\n * must be in expanded form before this method is called.\n *\n * @param activeCtx the active context to use.\n * @param activeProperty the compacted property associated with the element\n *          to compact, null for none.\n * @param element the element to compact.\n * @param options the compaction options.\n * @param compactionMap the compaction map to use.\n *\n * @return the compacted value.\n */\napi.compact = ({\n  activeCtx,\n  activeProperty = null,\n  element,\n  options = {},\n  compactionMap = () => undefined\n}) => {\n  // recursively compact array\n  if(_isArray(element)) {\n    let rval = [];\n    for(let i = 0; i < element.length; ++i) {\n      // compact, dropping any null values unless custom mapped\n      let compacted = api.compact({\n        activeCtx,\n        activeProperty,\n        element: element[i],\n        options,\n        compactionMap\n      });\n      if(compacted === null) {\n        // TODO: use `await` to support async\n        compacted = compactionMap({\n          unmappedValue: element[i],\n          activeCtx,\n          activeProperty,\n          parent: element,\n          index: i,\n          options\n        });\n        if(compacted === undefined) {\n          continue;\n        }\n      }\n      rval.push(compacted);\n    }\n    if(options.compactArrays && rval.length === 1) {\n      // use single element if no container is specified\n      const container = _getContextValue(\n        activeCtx, activeProperty, '@container') || [];\n      if(container.length === 0) {\n        rval = rval[0];\n      }\n    }\n    return rval;\n  }\n\n  // use any scoped context on activeProperty\n  const ctx = _getContextValue(activeCtx, activeProperty, '@context');\n  if(ctx) {\n    activeCtx = _processContext({activeCtx, localCtx: ctx, options});\n  }\n\n  // recursively compact object\n  if(_isObject(element)) {\n    if(options.link && '@id' in element && element['@id'] in options.link) {\n      // check for a linked element to reuse\n      const linked = options.link[element['@id']];\n      for(let i = 0; i < linked.length; ++i) {\n        if(linked[i].expanded === element) {\n          return linked[i].compacted;\n        }\n      }\n    }\n\n    // do value compaction on @values and subject references\n    if(_isValue(element) || _isSubjectReference(element)) {\n      const rval =\n        api.compactValue({activeCtx, activeProperty, value: element});\n      if(options.link && _isSubjectReference(element)) {\n        // store linked element\n        if(!(element['@id'] in options.link)) {\n          options.link[element['@id']] = [];\n        }\n        options.link[element['@id']].push({expanded: element, compacted: rval});\n      }\n      return rval;\n    }\n\n    // FIXME: avoid misuse of active property as an expanded property?\n    const insideReverse = (activeProperty === '@reverse');\n\n    const rval = {};\n\n    if(options.link && '@id' in element) {\n      // store linked element\n      if(!(element['@id'] in options.link)) {\n        options.link[element['@id']] = [];\n      }\n      options.link[element['@id']].push({expanded: element, compacted: rval});\n    }\n\n    // apply any context defined on an alias of @type\n    // if key is @type and any compacted value is a term having a local\n    // context, overlay that context\n    const types = element['@type'] || [];\n    for(const type of types) {\n      const compactedType = api.compactIri(\n        {activeCtx, iri: type, relativeTo: {vocab: true}});\n\n      // Use any scoped context defined on this value\n      const ctx = _getContextValue(activeCtx, compactedType, '@context');\n      if(ctx) {\n        activeCtx = _processContext({activeCtx, localCtx: ctx, options});\n      }\n    }\n\n    // process element keys in order\n    const keys = Object.keys(element).sort();\n    for(const expandedProperty of keys) {\n      const expandedValue = element[expandedProperty];\n\n      // compact @id and @type(s)\n      if(expandedProperty === '@id' || expandedProperty === '@type') {\n        let compactedValue = [].concat(expandedValue).map(\n          expandedIri => api.compactIri({\n            activeCtx,\n            iri: expandedIri,\n            relativeTo: {\n              vocab: expandedProperty === '@type'\n            }\n          }));\n        if(compactedValue.length === 1) {\n          compactedValue = compactedValue[0];\n        }\n\n        // use keyword alias and add value\n        const alias = api.compactIri(\n          {activeCtx, iri: expandedProperty, relativeTo: {vocab: true}});\n        const isArray = _isArray(compactedValue) && expandedValue.length === 0;\n        _addValue(rval, alias, compactedValue, {propertyIsArray: isArray});\n        continue;\n      }\n\n      // handle @reverse\n      if(expandedProperty === '@reverse') {\n        // recursively compact expanded value\n        const compactedValue = api.compact({\n          activeCtx,\n          activeProperty: '@reverse',\n          element: expandedValue,\n          options,\n          compactionMap\n        });\n\n        // handle double-reversed properties\n        for(const compactedProperty in compactedValue) {\n          if(activeCtx.mappings[compactedProperty] &&\n            activeCtx.mappings[compactedProperty].reverse) {\n            const value = compactedValue[compactedProperty];\n            const container = _getContextValue(\n              activeCtx, compactedProperty, '@container') || [];\n            const useArray = (\n              container.includes('@set') || !options.compactArrays);\n            _addValue(\n              rval, compactedProperty, value, {propertyIsArray: useArray});\n            delete compactedValue[compactedProperty];\n          }\n        }\n\n        if(Object.keys(compactedValue).length > 0) {\n          // use keyword alias and add value\n          const alias = api.compactIri({\n            activeCtx,\n            iri: expandedProperty,\n            relativeTo: {vocab: true}\n          });\n          _addValue(rval, alias, compactedValue);\n        }\n\n        continue;\n      }\n\n      if(expandedProperty === '@preserve') {\n        // compact using activeProperty\n        const compactedValue = api.compact({\n          activeCtx,\n          activeProperty,\n          element: expandedValue,\n          options,\n          compactionMap});\n\n        if(!(_isArray(compactedValue) && compactedValue.length === 0)) {\n          _addValue(rval, expandedProperty, compactedValue);\n        }\n        continue;\n      }\n\n      // handle @index property\n      if(expandedProperty === '@index') {\n        // drop @index if inside an @index container\n        const container = _getContextValue(\n          activeCtx, activeProperty, '@container') || [];\n        if(container.includes('@index')) {\n          continue;\n        }\n\n        // use keyword alias and add value\n        const alias = api.compactIri({\n          activeCtx,\n          iri: expandedProperty,\n          relativeTo: {vocab: true}\n        });\n        _addValue(rval, alias, expandedValue);\n        continue;\n      }\n\n      // skip array processing for keywords that aren't @graph or @list\n      if(expandedProperty !== '@graph' && expandedProperty !== '@list' &&\n        _isKeyword(expandedProperty)) {\n        // use keyword alias and add value as is\n        const alias = api.compactIri({\n          activeCtx,\n          iri: expandedProperty,\n          relativeTo: {vocab: true}\n        });\n        _addValue(rval, alias, expandedValue);\n        continue;\n      }\n\n      // Note: expanded value must be an array due to expansion algorithm.\n      if(!_isArray(expandedValue)) {\n        throw new JsonLdError(\n          'JSON-LD expansion error; expanded value must be an array.',\n          'jsonld.SyntaxError');\n      }\n\n      // preserve empty arrays\n      if(expandedValue.length === 0) {\n        const itemActiveProperty = api.compactIri({\n          activeCtx,\n          iri: expandedProperty,\n          value: expandedValue,\n          relativeTo: {vocab: true},\n          reverse: insideReverse\n        });\n        const nestProperty = (itemActiveProperty in activeCtx.mappings) ?\n          activeCtx.mappings[itemActiveProperty]['@nest'] : null;\n        let nestResult = rval;\n        if(nestProperty) {\n          _checkNestProperty(activeCtx, nestProperty);\n          if(!_isObject(rval[nestProperty])) {\n            rval[nestProperty] = {};\n          }\n          nestResult = rval[nestProperty];\n        }\n        _addValue(\n          nestResult, itemActiveProperty, expandedValue, {\n            propertyIsArray: true\n          });\n      }\n\n      // recusively process array values\n      for(const expandedItem of expandedValue) {\n        // compact property and get container type\n        const itemActiveProperty = api.compactIri({\n          activeCtx,\n          iri: expandedProperty,\n          value: expandedItem,\n          relativeTo: {vocab: true},\n          reverse: insideReverse\n        });\n\n        // if itemActiveProperty is a @nest property, add values to nestResult,\n        // otherwise rval\n        const nestProperty = (itemActiveProperty in activeCtx.mappings) ?\n          activeCtx.mappings[itemActiveProperty]['@nest'] : null;\n        let nestResult = rval;\n        if(nestProperty) {\n          _checkNestProperty(activeCtx, nestProperty);\n          if(!_isObject(rval[nestProperty])) {\n            rval[nestProperty] = {};\n          }\n          nestResult = rval[nestProperty];\n        }\n\n        const container = _getContextValue(\n          activeCtx, itemActiveProperty, '@container') || [];\n\n        // get simple @graph or @list value if appropriate\n        const isGraph = _isGraph(expandedItem);\n        const isList = _isList(expandedItem);\n        let inner;\n        if(isList) {\n          inner = expandedItem['@list'];\n        } else if(isGraph) {\n          inner = expandedItem['@graph'];\n        }\n\n        // recursively compact expanded item\n        let compactedItem = api.compact({\n          activeCtx,\n          activeProperty: itemActiveProperty,\n          element: (isList || isGraph) ? inner : expandedItem,\n          options,\n          compactionMap\n        });\n\n        // handle @list\n        if(isList) {\n          // ensure @list value is an array\n          if(!_isArray(compactedItem)) {\n            compactedItem = [compactedItem];\n          }\n\n          if(!container.includes('@list')) {\n            // wrap using @list alias\n            compactedItem = {\n              [api.compactIri({\n                activeCtx,\n                iri: '@list',\n                relativeTo: {vocab: true}\n              })]: compactedItem\n            };\n\n            // include @index from expanded @list, if any\n            if('@index' in expandedItem) {\n              compactedItem[api.compactIri({\n                activeCtx,\n                iri: '@index',\n                relativeTo: {vocab: true}\n              })] = expandedItem['@index'];\n            }\n          } else if(itemActiveProperty in nestResult) {\n            // can't use @list container for more than 1 list\n            throw new JsonLdError(\n              'JSON-LD compact error; property has a \"@list\" @container ' +\n              'rule but there is more than a single @list that matches ' +\n              'the compacted term in the document. Compaction might mix ' +\n              'unwanted items into the list.',\n              'jsonld.SyntaxError', {code: 'compaction to list of lists'});\n          }\n        }\n\n        // Graph object compaction cases\n        if(isGraph) {\n          if(container.includes('@graph') && (container.includes('@id') ||\n            container.includes('@index') && _isSimpleGraph(expandedItem))) {\n            // get or create the map object\n            let mapObject;\n            if(itemActiveProperty in nestResult) {\n              mapObject = nestResult[itemActiveProperty];\n            } else {\n              nestResult[itemActiveProperty] = mapObject = {};\n            }\n\n            // index on @id or @index or alias of @none\n            const key = (container.includes('@id') ?\n              expandedItem['@id'] : expandedItem['@index']) ||\n              api.compactIri({activeCtx, iri: '@none', vocab: true});\n            // add compactedItem to map, using value of `@id` or a new blank\n            // node identifier\n\n            _addValue(\n              mapObject, key, compactedItem, {\n                propertyIsArray:\n                  (!options.compactArrays || container.includes('@set'))\n              });\n          } else if(container.includes('@graph') &&\n            _isSimpleGraph(expandedItem)) {\n            // container includes @graph but not @id or @index and value is a\n            // simple graph object add compact value\n            _addValue(\n              nestResult, itemActiveProperty, compactedItem, {\n                propertyIsArray:\n                  (!options.compactArrays || container.includes('@set'))\n              });\n          } else {\n            // wrap using @graph alias, remove array if only one item and\n            // compactArrays not set\n            if(_isArray(compactedItem) && compactedItem.length === 1 &&\n              options.compactArrays) {\n              compactedItem = compactedItem[0];\n            }\n            compactedItem = {\n              [api.compactIri({\n                activeCtx,\n                iri: '@graph',\n                relativeTo: {vocab: true}\n              })]: compactedItem\n            };\n\n            // include @id from expanded graph, if any\n            if('@id' in expandedItem) {\n              compactedItem[api.compactIri({\n                activeCtx,\n                iri: '@id',\n                relativeTo: {vocab: true}\n              })] = expandedItem['@id'];\n            }\n\n            // include @index from expanded graph, if any\n            if('@index' in expandedItem) {\n              compactedItem[api.compactIri({\n                activeCtx,\n                iri: '@index',\n                relativeTo: {vocab: true}\n              })] = expandedItem['@index'];\n            }\n            _addValue(\n              nestResult, itemActiveProperty, compactedItem, {\n                propertyIsArray:\n                  (!options.compactArrays || container.includes('@set'))\n              });\n          }\n        } else if(container.includes('@language') ||\n          container.includes('@index') || container.includes('@id') ||\n          container.includes('@type')) {\n          // handle language and index maps\n          // get or create the map object\n          let mapObject;\n          if(itemActiveProperty in nestResult) {\n            mapObject = nestResult[itemActiveProperty];\n          } else {\n            nestResult[itemActiveProperty] = mapObject = {};\n          }\n\n          let key;\n          if(container.includes('@language')) {\n          // if container is a language map, simplify compacted value to\n          // a simple string\n            if(_isValue(compactedItem)) {\n              compactedItem = compactedItem['@value'];\n            }\n            key = expandedItem['@language'];\n          } else if(container.includes('@index')) {\n            key = expandedItem['@index'];\n          } else if(container.includes('@id')) {\n            const idKey = api.compactIri({activeCtx, iri: '@id', vocab: true});\n            key = compactedItem[idKey];\n            delete compactedItem[idKey];\n          } else if(container.includes('@type')) {\n            const typeKey = api.compactIri({\n              activeCtx,\n              iri: '@type',\n              vocab: true\n            });\n            let types;\n            [key, ...types] = [].concat(compactedItem[typeKey] || []);\n            switch(types.length) {\n            case 0:\n              delete compactedItem[typeKey];\n              break;\n            case 1:\n              compactedItem[typeKey] = types[0];\n              break;\n            default:\n              compactedItem[typeKey] = types;\n              break;\n            }\n          }\n\n          // if compacting this value which has no key, index on @none\n          if(!key) {\n            key = api.compactIri({activeCtx, iri: '@none', vocab: true});\n          }\n          // add compact value to map object using key from expanded value\n          // based on the container type\n          _addValue(\n            mapObject, key, compactedItem, {\n              propertyIsArray: container.includes('@set')\n            });\n        } else {\n          // use an array if: compactArrays flag is false,\n          // @container is @set or @list , value is an empty\n          // array, or key is @graph\n          const isArray = (!options.compactArrays ||\n            container.includes('@set') || container.includes('@list') ||\n            (_isArray(compactedItem) && compactedItem.length === 0) ||\n            expandedProperty === '@list' || expandedProperty === '@graph');\n\n          // add compact value\n          _addValue(\n            nestResult, itemActiveProperty, compactedItem,\n            {propertyIsArray: isArray});\n        }\n      }\n    }\n\n    return rval;\n  }\n\n  // only primitives remain which are already compact\n  return element;\n};\n\n/**\n * Compacts an IRI or keyword into a term or prefix if it can be. If the\n * IRI has an associated value it may be passed.\n *\n * @param activeCtx the active context to use.\n * @param iri the IRI to compact.\n * @param value the value to check or null.\n * @param relativeTo options for how to compact IRIs:\n *          vocab: true to split after @vocab, false not to.\n * @param reverse true if a reverse property is being compacted, false if not.\n *\n * @return the compacted term, prefix, keyword alias, or the original IRI.\n */\napi.compactIri = ({\n  activeCtx,\n  iri,\n  value = null,\n  relativeTo = {vocab: false},\n  reverse = false\n}) => {\n  // can't compact null\n  if(iri === null) {\n    return iri;\n  }\n\n  const inverseCtx = activeCtx.getInverse();\n\n  // if term is a keyword, it may be compacted to a simple alias\n  if(_isKeyword(iri) &&\n    iri in inverseCtx &&\n    '@none' in inverseCtx[iri] &&\n    '@type' in inverseCtx[iri]['@none'] &&\n    '@none' in inverseCtx[iri]['@none']['@type']) {\n    return inverseCtx[iri]['@none']['@type']['@none'];\n  }\n\n  // use inverse context to pick a term if iri is relative to vocab\n  if(relativeTo.vocab && iri in inverseCtx) {\n    const defaultLanguage = activeCtx['@language'] || '@none';\n\n    // prefer @index if available in value\n    const containers = [];\n    if(_isObject(value) && '@index' in value && !('@graph' in value)) {\n      containers.push('@index', '@index@set');\n    }\n\n    // if value is a preserve object, use its value\n    if(_isObject(value) && '@preserve' in value) {\n      value = value['@preserve'][0];\n    }\n\n    // prefer most specific container including @graph, prefering @set\n    // variations\n    if(_isGraph(value)) {\n      // favor indexmap if the graph is indexed\n      if('@index' in value) {\n        containers.push(\n          '@graph@index', '@graph@index@set', '@index', '@index@set');\n      }\n      // favor idmap if the graph is has an @id\n      if('@id' in value) {\n        containers.push(\n          '@graph@id', '@graph@id@set');\n      }\n      containers.push('@graph', '@graph@set', '@set');\n      // allow indexmap if the graph is not indexed\n      if(!('@index' in value)) {\n        containers.push(\n          '@graph@index', '@graph@index@set', '@index', '@index@set');\n      }\n      // allow idmap if the graph does not have an @id\n      if(!('@id' in value)) {\n        containers.push('@graph@id', '@graph@id@set');\n      }\n    } else if(_isObject(value) && !_isValue(value)) {\n      containers.push('@id', '@id@set', '@type', '@set@type');\n    }\n\n    // defaults for term selection based on type/language\n    let typeOrLanguage = '@language';\n    let typeOrLanguageValue = '@null';\n\n    if(reverse) {\n      typeOrLanguage = '@type';\n      typeOrLanguageValue = '@reverse';\n      containers.push('@set');\n    } else if(_isList(value)) {\n      // choose the most specific term that works for all elements in @list\n      // only select @list containers if @index is NOT in value\n      if(!('@index' in value)) {\n        containers.push('@list');\n      }\n      const list = value['@list'];\n      if(list.length === 0) {\n        // any empty list can be matched against any term that uses the\n        // @list container regardless of @type or @language\n        typeOrLanguage = '@any';\n        typeOrLanguageValue = '@none';\n      } else {\n        let commonLanguage = (list.length === 0) ? defaultLanguage : null;\n        let commonType = null;\n        for(let i = 0; i < list.length; ++i) {\n          const item = list[i];\n          let itemLanguage = '@none';\n          let itemType = '@none';\n          if(_isValue(item)) {\n            if('@language' in item) {\n              itemLanguage = item['@language'];\n            } else if('@type' in item) {\n              itemType = item['@type'];\n            } else {\n              // plain literal\n              itemLanguage = '@null';\n            }\n          } else {\n            itemType = '@id';\n          }\n          if(commonLanguage === null) {\n            commonLanguage = itemLanguage;\n          } else if(itemLanguage !== commonLanguage && _isValue(item)) {\n            commonLanguage = '@none';\n          }\n          if(commonType === null) {\n            commonType = itemType;\n          } else if(itemType !== commonType) {\n            commonType = '@none';\n          }\n          // there are different languages and types in the list, so choose\n          // the most generic term, no need to keep iterating the list\n          if(commonLanguage === '@none' && commonType === '@none') {\n            break;\n          }\n        }\n        commonLanguage = commonLanguage || '@none';\n        commonType = commonType || '@none';\n        if(commonType !== '@none') {\n          typeOrLanguage = '@type';\n          typeOrLanguageValue = commonType;\n        } else {\n          typeOrLanguageValue = commonLanguage;\n        }\n      }\n    } else {\n      if(_isValue(value)) {\n        if('@language' in value && !('@index' in value)) {\n          containers.push('@language', '@language@set');\n          typeOrLanguageValue = value['@language'];\n        } else if('@type' in value) {\n          typeOrLanguage = '@type';\n          typeOrLanguageValue = value['@type'];\n        }\n      } else {\n        typeOrLanguage = '@type';\n        typeOrLanguageValue = '@id';\n      }\n      containers.push('@set');\n    }\n\n    // do term selection\n    containers.push('@none');\n\n    // an index map can be used to index values using @none, so add as a low\n    // priority\n    if(_isObject(value) && !('@index' in value)) {\n      // allow indexing even if no @index present\n      containers.push('@index', '@index@set');\n    }\n\n    // values without type or language can use @language map\n    if(_isValue(value) && Object.keys(value).length === 1) {\n      // allow indexing even if no @index present\n      containers.push('@language', '@language@set');\n    }\n\n    const term = _selectTerm(\n      activeCtx, iri, value, containers, typeOrLanguage, typeOrLanguageValue);\n    if(term !== null) {\n      return term;\n    }\n  }\n\n  // no term match, use @vocab if available\n  if(relativeTo.vocab) {\n    if('@vocab' in activeCtx) {\n      // determine if vocab is a prefix of the iri\n      const vocab = activeCtx['@vocab'];\n      if(iri.indexOf(vocab) === 0 && iri !== vocab) {\n        // use suffix as relative iri if it is not a term in the active context\n        const suffix = iri.substr(vocab.length);\n        if(!(suffix in activeCtx.mappings)) {\n          return suffix;\n        }\n      }\n    }\n  }\n\n  // no term or @vocab match, check for possible CURIEs\n  let choice = null;\n  // TODO: make FastCurieMap a class with a method to do this lookup\n  const partialMatches = [];\n  let iriMap = activeCtx.fastCurieMap;\n  // check for partial matches of against `iri`, which means look until\n  // iri.length - 1, not full length\n  const maxPartialLength = iri.length - 1;\n  for(let i = 0; i < maxPartialLength && iri[i] in iriMap; ++i) {\n    iriMap = iriMap[iri[i]];\n    if('' in iriMap) {\n      partialMatches.push(iriMap[''][0]);\n    }\n  }\n  // check partial matches in reverse order to prefer longest ones first\n  for(let i = partialMatches.length - 1; i >= 0; --i) {\n    const entry = partialMatches[i];\n    const terms = entry.terms;\n    for(const term of terms) {\n      // a CURIE is usable if:\n      // 1. it has no mapping, OR\n      // 2. value is null, which means we're not compacting an @value, AND\n      //   the mapping matches the IRI\n      const curie = term + ':' + iri.substr(entry.iri.length);\n      const isUsableCurie = (activeCtx.mappings[term]._prefix &&\n        (!(curie in activeCtx.mappings) ||\n        (value === null && activeCtx.mappings[curie]['@id'] === iri)));\n\n      // select curie if it is shorter or the same length but lexicographically\n      // less than the current choice\n      if(isUsableCurie && (choice === null ||\n        _compareShortestLeast(curie, choice) < 0)) {\n        choice = curie;\n      }\n    }\n  }\n\n  // return chosen curie\n  if(choice !== null) {\n    return choice;\n  }\n\n  // compact IRI relative to base\n  if(!relativeTo.vocab) {\n    return _removeBase(activeCtx['@base'], iri);\n  }\n\n  // return IRI as is\n  return iri;\n};\n\n/**\n * Performs value compaction on an object with '@value' or '@id' as the only\n * property.\n *\n * @param activeCtx the active context.\n * @param activeProperty the active property that points to the value.\n * @param value the value to compact.\n *\n * @return the compaction result.\n */\napi.compactValue = ({activeCtx, activeProperty, value}) => {\n  // value is a @value\n  if(_isValue(value)) {\n    // get context rules\n    const type = _getContextValue(activeCtx, activeProperty, '@type');\n    const language = _getContextValue(activeCtx, activeProperty, '@language');\n    const container =\n      _getContextValue(activeCtx, activeProperty, '@container') || [];\n\n    // whether or not the value has an @index that must be preserved\n    const preserveIndex = '@index' in value && !container.includes('@index');\n\n    // if there's no @index to preserve ...\n    if(!preserveIndex) {\n      // matching @type or @language specified in context, compact value\n      if(value['@type'] === type || value['@language'] === language) {\n        return value['@value'];\n      }\n    }\n\n    // return just the value of @value if all are true:\n    // 1. @value is the only key or @index isn't being preserved\n    // 2. there is no default language or @value is not a string or\n    //   the key has a mapping with a null @language\n    const keyCount = Object.keys(value).length;\n    const isValueOnlyKey = (keyCount === 1 ||\n      (keyCount === 2 && '@index' in value && !preserveIndex));\n    const hasDefaultLanguage = ('@language' in activeCtx);\n    const isValueString = _isString(value['@value']);\n    const hasNullMapping = (activeCtx.mappings[activeProperty] &&\n      activeCtx.mappings[activeProperty]['@language'] === null);\n    if(isValueOnlyKey &&\n      (!hasDefaultLanguage || !isValueString || hasNullMapping)) {\n      return value['@value'];\n    }\n\n    const rval = {};\n\n    // preserve @index\n    if(preserveIndex) {\n      rval[api.compactIri({\n        activeCtx,\n        iri: '@index',\n        relativeTo: {vocab: true}\n      })] = value['@index'];\n    }\n\n    if('@type' in value) {\n      // compact @type IRI\n      rval[api.compactIri({\n        activeCtx,\n        iri: '@type',\n        relativeTo: {vocab: true}\n      })] = api.compactIri(\n        {activeCtx, iri: value['@type'], relativeTo: {vocab: true}});\n    } else if('@language' in value) {\n      // alias @language\n      rval[api.compactIri({\n        activeCtx,\n        iri: '@language',\n        relativeTo: {vocab: true}\n      })] = value['@language'];\n    }\n\n    // alias @value\n    rval[api.compactIri({\n      activeCtx,\n      iri: '@value',\n      relativeTo: {vocab: true}\n    })] = value['@value'];\n\n    return rval;\n  }\n\n  // value is a subject reference\n  const expandedProperty = _expandIri(activeCtx, activeProperty, {vocab: true});\n  const type = _getContextValue(activeCtx, activeProperty, '@type');\n  const compacted = api.compactIri(\n    {activeCtx, iri: value['@id'], relativeTo: {vocab: type === '@vocab'}});\n\n  // compact to scalar\n  if(type === '@id' || type === '@vocab' || expandedProperty === '@graph') {\n    return compacted;\n  }\n\n  return {\n    [api.compactIri({\n      activeCtx,\n      iri: '@id',\n      relativeTo: {vocab: true}\n    })]: compacted\n  };\n};\n\n/**\n * Removes the @preserve keywords as the last step of the compaction\n * algorithm when it is running on framed output.\n *\n * @param ctx the active context used to compact the input.\n * @param input the framed, compacted output.\n * @param options the compaction options used.\n *\n * @return the resulting output.\n */\napi.removePreserve = (ctx, input, options) => {\n  // recurse through arrays\n  if(_isArray(input)) {\n    const output = [];\n    for(let i = 0; i < input.length; ++i) {\n      const result = api.removePreserve(ctx, input[i], options);\n      // drop nulls from arrays\n      if(result !== null) {\n        output.push(result);\n      }\n    }\n    input = output;\n  } else if(_isObject(input)) {\n    // remove @preserve\n    if('@preserve' in input) {\n      if(input['@preserve'] === '@null') {\n        return null;\n      }\n      return input['@preserve'];\n    }\n\n    // skip @values\n    if(_isValue(input)) {\n      return input;\n    }\n\n    // recurse through @lists\n    if(_isList(input)) {\n      input['@list'] = api.removePreserve(ctx, input['@list'], options);\n      return input;\n    }\n\n    // handle in-memory linked nodes\n    const idAlias = api.compactIri({\n      activeCtx: ctx,\n      iri: '@id',\n      relativeTo: {vocab: true}\n    });\n    if(idAlias in input) {\n      const id = input[idAlias];\n      if(id in options.link) {\n        const idx = options.link[id].indexOf(input);\n        if(idx !== -1) {\n          // already visited\n          return options.link[id][idx];\n        }\n        // prevent circular visitation\n        options.link[id].push(input);\n      } else {\n        // prevent circular visitation\n        options.link[id] = [input];\n      }\n    }\n\n    // recurse through properties\n    const graphAlias = api.compactIri({\n      activeCtx: ctx,\n      iri: '@graph',\n      relativeTo: {vocab: true}\n    });\n    for(const prop in input) {\n      // potentially remove the id, if it is an unreference bnode\n      if(prop === idAlias && options.bnodesToClear.includes(input[prop])) {\n        delete input[idAlias];\n        continue;\n      }\n\n      let result = api.removePreserve(ctx, input[prop], options);\n      const container = _getContextValue(ctx, prop, '@container') || [];\n      if(options.compactArrays && _isArray(result) && result.length === 1 &&\n        container.length === 0 && prop !== graphAlias) {\n        result = result[0];\n      }\n      input[prop] = result;\n    }\n  }\n  return input;\n};\n\n/**\n * Picks the preferred compaction term from the given inverse context entry.\n *\n * @param activeCtx the active context.\n * @param iri the IRI to pick the term for.\n * @param value the value to pick the term for.\n * @param containers the preferred containers.\n * @param typeOrLanguage either '@type' or '@language'.\n * @param typeOrLanguageValue the preferred value for '@type' or '@language'.\n *\n * @return the preferred term.\n */\nfunction _selectTerm(\n  activeCtx, iri, value, containers, typeOrLanguage, typeOrLanguageValue) {\n  if(typeOrLanguageValue === null) {\n    typeOrLanguageValue = '@null';\n  }\n\n  // preferences for the value of @type or @language\n  const prefs = [];\n\n  // determine prefs for @id based on whether or not value compacts to a term\n  if((typeOrLanguageValue === '@id' || typeOrLanguageValue === '@reverse') &&\n    _isSubjectReference(value)) {\n    // prefer @reverse first\n    if(typeOrLanguageValue === '@reverse') {\n      prefs.push('@reverse');\n    }\n    // try to compact value to a term\n    const term = api.compactIri(\n      {activeCtx, iri: value['@id'], relativeTo: {vocab: true}});\n    if(term in activeCtx.mappings &&\n      activeCtx.mappings[term] &&\n      activeCtx.mappings[term]['@id'] === value['@id']) {\n      // prefer @vocab\n      prefs.push.apply(prefs, ['@vocab', '@id']);\n    } else {\n      // prefer @id\n      prefs.push.apply(prefs, ['@id', '@vocab']);\n    }\n  } else {\n    prefs.push(typeOrLanguageValue);\n  }\n  prefs.push('@none');\n\n  const containerMap = activeCtx.inverse[iri];\n  for(let ci = 0; ci < containers.length; ++ci) {\n    // if container not available in the map, continue\n    const container = containers[ci];\n    if(!(container in containerMap)) {\n      continue;\n    }\n\n    const typeOrLanguageValueMap = containerMap[container][typeOrLanguage];\n    for(let pi = 0; pi < prefs.length; ++pi) {\n      // if type/language option not available in the map, continue\n      const pref = prefs[pi];\n      if(!(pref in typeOrLanguageValueMap)) {\n        continue;\n      }\n\n      // select term\n      return typeOrLanguageValueMap[pref];\n    }\n  }\n\n  return null;\n}\n\n/**\n * The value of `@nest` in the term definition must either be `@nest`, or a term\n * which resolves to `@nest`.\n *\n * @param activeCtx the active context.\n * @param nestProperty a term in the active context or `@nest`.\n */\nfunction _checkNestProperty(activeCtx, nestProperty) {\n  if(_expandIri(activeCtx, nestProperty, {vocab: true}) !== '@nest') {\n    throw new JsonLdError(\n      'JSON-LD compact error; nested property must have an @nest value ' +\n      'resolving to @nest.',\n      'jsonld.SyntaxError', {code: 'invalid @nest value'});\n  }\n}\n"],"mappings":"AACA;;;;;;;ACSA;AC4gBA;AAAA;;;;;;;;;;;;;;ACtgBA;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;ACmBA;AA4MA;AAAA;AC3OA;ACHA;;;;;;;;;;ACk4BA;;AAcA","sourceRoot":""}